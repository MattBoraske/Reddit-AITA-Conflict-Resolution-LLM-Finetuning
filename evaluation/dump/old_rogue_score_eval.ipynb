{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Metric\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "def evaluate_model(model, sample):\n",
    "\n",
    "    # tokenize input\n",
    "    input_text = sample[\"submission_text\"]\n",
    "    input_ids = tokenizer(INSTRUCTION_PREFIX + input_text, max_length=FLAN_T5_ENCODER_CONTEXT_WINDOW_SIZE, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n",
    "\n",
    "    # generate and decode prediction\n",
    "    outputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9, max_new_tokens=FLAN_T5_DECODER_CONTEXT_WINDOW_SIZE)\n",
    "    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n",
    "\n",
    "    # get label\n",
    "    label = sample['top_comment_1']\n",
    "\n",
    "    # return prediction and label\n",
    "    return input_text, prediction, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load first N samples in test dataset\n",
    "NUMBER_OF_SAMPLES = 100\n",
    "test_dataset = dataset['test'].select(range(NUMBER_OF_SAMPLES))\n",
    "\n",
    "# run predictions\n",
    "input_texts, predictions, references = [] , [], []\n",
    "for sample in tqdm(test_dataset):\n",
    "    i,p,l = evaluate_model(peft_model, sample)\n",
    "    input_texts.append(i)\n",
    "    predictions.append(p)\n",
    "    references.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute ROGUE scores\n",
    "rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n",
    "\n",
    "rouge_scores = {\n",
    "    'ROUGE-1': f\"{rogue['rouge1'] * 100:.2f}%\",\n",
    "    'ROUGE-2': f\"{rogue['rouge2'] * 100:.2f}%\",\n",
    "    'ROUGE-L': f\"{rogue['rougeL'] * 100:.2f}%\",\n",
    "    'ROUGE-Lsum': f\"{rogue['rougeLsum'] * 100:.2f}%\"\n",
    "}\n",
    "\n",
    "print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n",
    "print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n",
    "print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n",
    "print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "INSTRUCTION_PREFIX = \"Classify the interpersonal conflict into one of the following categories. 'YTA' when the writer is causing the conflict. 'NTA' when the other person is causing the conflict. 'NAH' when both the writer and other person are not causing the conflict. 'ESH' when both the writer and other person are causing the conflict. 'INFO' if more information is needed for a judgement. Then, provide a short justification: \"\n",
    "\n",
    "results = {}\n",
    "for i, (input_text, prediction, reference) in enumerate(zip(input_texts, predictions, references)):\n",
    "    results[f'Sample {i+1}'] = {'Input Text': input_text, 'Prediction': prediction, 'Reference': reference}\n",
    "\n",
    "final_output = {\n",
    "    'Instruction Prefix': INSTRUCTION_PREFIX,\n",
    "    'ROUGE Scores': rouge_scores,\n",
    "    'Results': results,\n",
    "}\n",
    "\n",
    "with open('/content/drive/MyDrive/WCU_THESIS/AITA_Fine_Tuning/flanT5_xxl_400_samples_training_100_testing_samples_results.json', 'w') as file:\n",
    "    json.dump(final_output, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
