{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip install evaluate rouge_score"]},{"cell_type":"markdown","metadata":{},"source":["# Stuff from other notebooks"]},{"cell_type":"markdown","metadata":{},"source":["- make sure to load flan-t5 base model in bfloat16 before adding peft adapter"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from datasets import load_dataset\n","dataset = load_dataset(\"MattBoraske/reddit-AITA-submissions-and-comments-top-2500\")"]},{"cell_type":"markdown","metadata":{},"source":["## Single sample test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from random import randrange\n","sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n","sample"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["input_ids = tokenizer(sample['flanT5_instruction'], max_length=FLAN_T5_ENCODER_CONTEXT_WINDOW_SIZE, padding='max_length', return_tensors=\"pt\", truncation=True).input_ids.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["outputs = peft_model.generate(\n","  input_ids=input_ids,\n","  max_new_tokens=FLAN_T5_DECODER_CONTEXT_WINDOW_SIZE,\n","  repetition_penalty=1.4\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n","print(f\"Prediction:\\n{prediction}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import evaluate\n","import numpy as np\n","from datasets import load_from_disk\n","from tqdm import tqdm\n","\n","# Metric\n","metric = evaluate.load(\"rouge\")\n","\n","def evaluate_model(model, sample):\n","\n","    # tokenize input\n","    input_text = sample[\"submission_text\"]\n","    input_ids = tokenizer(INSTRUCTION_PREFIX + input_text, max_length=FLAN_T5_ENCODER_CONTEXT_WINDOW_SIZE, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n","\n","    # generate and decode prediction\n","    outputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9, max_new_tokens=FLAN_T5_DECODER_CONTEXT_WINDOW_SIZE)\n","    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n","\n","    # get label\n","    label = sample['top_comment_1']\n","\n","    # return prediction and label\n","    return input_text, prediction, label"]},{"cell_type":"markdown","metadata":{},"source":["## ROGUE Score Testing Loop"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import evaluate\n","import numpy as np\n","from datasets import load_from_disk\n","from tqdm import tqdm\n","\n","# Metric\n","metric = evaluate.load(\"rouge\")\n","\n","def evaluate_model(model, sample):\n","\n","    # tokenize input\n","    input_text = sample[\"submission_text\"]\n","    input_ids = tokenizer(INSTRUCTION_PREFIX + input_text, max_length=FLAN_T5_ENCODER_CONTEXT_WINDOW_SIZE, return_tensors=\"pt\", truncation=True).input_ids.cuda()\n","\n","    # generate and decode prediction\n","    outputs = model.generate(input_ids=input_ids, do_sample=True, top_p=0.9, max_new_tokens=FLAN_T5_DECODER_CONTEXT_WINDOW_SIZE)\n","    prediction = tokenizer.decode(outputs[0].detach().cpu().numpy(), skip_special_tokens=True)\n","\n","    # get label\n","    label = sample['top_comment_1']\n","\n","    # return prediction and label\n","    return input_text, prediction, label"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# load first N samples in test dataset\n","NUMBER_OF_SAMPLES = 100\n","test_dataset = dataset['test'].select(range(NUMBER_OF_SAMPLES))\n","\n","# run predictions\n","input_texts, predictions, references = [] , [], []\n","for sample in tqdm(test_dataset):\n","    i,p,l = evaluate_model(peft_model, sample)\n","    input_texts.append(i)\n","    predictions.append(p)\n","    references.append(l)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Compute ROGUE scores\n","rogue = metric.compute(predictions=predictions, references=references, use_stemmer=True)\n","\n","rouge_scores = {\n","    'ROUGE-1': f\"{rogue['rouge1'] * 100:.2f}%\",\n","    'ROUGE-2': f\"{rogue['rouge2'] * 100:.2f}%\",\n","    'ROUGE-L': f\"{rogue['rougeL'] * 100:.2f}%\",\n","    'ROUGE-Lsum': f\"{rogue['rougeLsum'] * 100:.2f}%\"\n","}\n","\n","print(f\"Rogue1: {rogue['rouge1']* 100:2f}%\")\n","print(f\"rouge2: {rogue['rouge2']* 100:2f}%\")\n","print(f\"rougeL: {rogue['rougeL']* 100:2f}%\")\n","print(f\"rougeLsum: {rogue['rougeLsum']* 100:2f}%\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import json\n","\n","INSTRUCTION_PREFIX = \"Classify the interpersonal conflict into one of the following categories. 'YTA' when the writer is causing the conflict. 'NTA' when the other person is causing the conflict. 'NAH' when both the writer and other person are not causing the conflict. 'ESH' when both the writer and other person are causing the conflict. 'INFO' if more information is needed for a judgement. Then, provide a short justification: \"\n","\n","results = {}\n","for i, (input_text, prediction, reference) in enumerate(zip(input_texts, predictions, references)):\n","    results[f'Sample {i+1}'] = {'Input Text': input_text, 'Prediction': prediction, 'Reference': reference}\n","\n","final_output = {\n","    'Instruction Prefix': INSTRUCTION_PREFIX,\n","    'ROUGE Scores': rouge_scores,\n","    'Results': results,\n","}\n","\n","with open('/content/drive/MyDrive/WCU_THESIS/AITA_Fine_Tuning/flanT5_xxl_400_samples_training_100_testing_samples_results.json', 'w') as file:\n","    json.dump(final_output, file, indent=4)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.11"}},"nbformat":4,"nbformat_minor":2}
