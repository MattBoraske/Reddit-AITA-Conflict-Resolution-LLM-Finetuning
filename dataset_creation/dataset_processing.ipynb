{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AITA Dataset Processing\n",
    "- Creates two datasets - top 50k and 2k by submission score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (2.16.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (4.37.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.5.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (1.23.3)\n",
      "Requirement already satisfied: krippendorff in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.6.1)\n",
      "Requirement already satisfied: huggingface_hub in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.20.3)\n",
      "Requirement already satisfied: ipywidgets in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (8.1.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (3.11.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (4.66.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (3.9.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas) (2022.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from huggingface_hub) (4.9.0)\n",
      "Requirement already satisfied: comm>=0.1.3 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets) (0.1.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets) (8.11.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets) (5.9.0)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.10 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets) (4.0.10)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.10 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipywidgets) (3.0.10)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: backcall in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.18.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (3.0.38)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (2.14.0)\n",
      "Requirement already satisfied: stack-data in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.6.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from ipython>=6.1.0->ipywidgets) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.19.0->datasets) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.1.0->ipywidgets) (0.2.6)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (1.2.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\mattb\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1 -> 24.0\n",
      "[notice] To update, run: C:\\Users\\mattb\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets transformers pandas numpy krippendorff huggingface_hub ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mattb\\Documents\\Github\\Reddit_AITA_Finetuning\\dataset_creation\n"
     ]
    }
   ],
   "source": [
    "# Change this to the path of the dataset_creation folder on your machine\n",
    "%cd C:\\Users\\mattb\\Documents\\Github\\Reddit_AITA_Finetuning\\dataset_creation\n",
    "!mkdir processing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47edcda2e27448269f54566e95bd8e48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of Initial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "initial_datafile = '2019_to_2022_submissions_at_least_50_score_top_10_comments.csv'  # change to data file path\n",
    "dataset = Dataset.from_pandas(pd.read_csv(initial_datafile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Samples where Top Comment Doesn't Begin with an AITA Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e54e421abe64cebbe039a5001d9cc59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/101603 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# regex pattern that matches rows where 'top_comment_1' starts with 'nta', 'yta', 'esh', 'info', or 'nah'\n",
    "regex = re.compile(r'^(nta|yta|esh|info|nah)', re.IGNORECASE)\n",
    "\n",
    "# function to apply the regex filter\n",
    "def filter_rows(example):\n",
    "    return bool(regex.match(example['top_comment_1']))\n",
    "\n",
    "# filter the dataset using the regex pattern\n",
    "filtered_dataset = dataset.filter(filter_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save AITA classification prefix filtering results\n",
    "\n",
    "rows_removed = dataset.num_rows - filtered_dataset.num_rows\n",
    "percent_change = (filtered_dataset.num_rows - dataset.num_rows) / dataset.num_rows * 100\n",
    "\n",
    "AITA_class_prefix_filtering_results = {\n",
    "    \"number of samples before filtering\": dataset.num_rows,\n",
    "    \"number of samples after filtering\": filtered_dataset.num_rows,\n",
    "    \"number of samples removed\": rows_removed,\n",
    "    \"percent change in number of samples\": percent_change,\n",
    "}\n",
    "\n",
    "output_file = \"processing_results/AITA_prefix_filtering_results.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(AITA_class_prefix_filtering_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = filtered_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Edits in both Submission Texts and Top Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_edits(text):\n",
    "  \"\"\"\n",
    "  Removes the edits portion of a text\n",
    "\n",
    "  Parameters:\n",
    "    text: A string containing the text.\n",
    "\n",
    "  Returns:\n",
    "    A string with the edits removed, if present.\n",
    "  \"\"\"\n",
    "\n",
    "  global edits_removed_counter\n",
    "\n",
    "  if text == None:\n",
    "    return text\n",
    "\n",
    "  text = text.lower()\n",
    "\n",
    "  pattern = r\"(edit:|edit -|edit-|eta:|eta -|eta-|edited:|edited -|edited-|edit after:|edit after- |edit after -|edit afterwards:|edit afterwards -|edit afterwards-|edited to add:|edited to add -|edited to add-|update:|update-|update -|updated:|updated-|updated -)\"\n",
    "  match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "  if match:\n",
    "      edits_removed_counter += 1 # increment the edits_removed_counter\n",
    "      return text[:match.start()].strip() # return the text up to the start of the match\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_length(strings):\n",
    "  \"\"\"\n",
    "  Calculates the average length of a list of strings.\n",
    "\n",
    "  Args:\n",
    "    strings (list): A list of strings.\n",
    "\n",
    "  Returns:\n",
    "    float: The average length of the strings.\n",
    "  \"\"\"\n",
    "\n",
    "  filtered_strings = [s for s in strings if s is not None] # filter out None values\n",
    "  total_length = sum(len(s) for s in filtered_strings)\n",
    "  average_length = total_length / len(filtered_strings) if filtered_strings else 0\n",
    "  return average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# create the results dictionary\n",
    "\n",
    "edits_removal_results = {\n",
    "    'submission_texts': defaultdict(list),\n",
    "    'top_comment_1': defaultdict(list),\n",
    "    'top_comment_2': defaultdict(list),\n",
    "    'top_comment_3': defaultdict(list),\n",
    "    'top_comment_4': defaultdict(list),\n",
    "    'top_comment_5': defaultdict(list),\n",
    "    'top_comment_6': defaultdict(list),\n",
    "    'top_comment_7': defaultdict(list),\n",
    "    'top_comment_8': defaultdict(list),\n",
    "    'top_comment_9': defaultdict(list),\n",
    "    'top_comment_10': defaultdict(list),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average lengths to result dictionary before removing edits\n",
    "\n",
    "texts_with_potential_edits = {\n",
    "    'submission_texts': dataset[\"submission_text\"],\n",
    "    'top_comment_1': dataset[\"top_comment_1\"],\n",
    "    'top_comment_2': dataset[\"top_comment_2\"],\n",
    "    'top_comment_3': dataset[\"top_comment_3\"],\n",
    "    'top_comment_4': dataset[\"top_comment_4\"],\n",
    "    'top_comment_5': dataset[\"top_comment_5\"],\n",
    "    'top_comment_6': dataset[\"top_comment_6\"],\n",
    "    'top_comment_7': dataset[\"top_comment_7\"],\n",
    "    'top_comment_8': dataset[\"top_comment_8\"],\n",
    "    'top_comment_9': dataset[\"top_comment_9\"],\n",
    "    'top_comment_10': dataset[\"top_comment_10\"],\n",
    "}\n",
    "\n",
    "for key, texts in texts_with_potential_edits.items():\n",
    "    edits_removal_results[key]['avg_length_before_removing_edits'] = get_avg_length(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63334941be474583a96ac76b5a283512",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67949521521456e99190502d69d7fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29b6658a94204b8b9b707cf4fd21d8b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b6ffbaf9deb4397a8b9d6a31a86106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f058d5b25a14d59bcce975a05dab856",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab542681f2834fb5bdabad764485dfd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411a88f1e6c64601a7a026d2fa9506e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c01a5de37849401dabb8017dda4d01c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "135e6a4f47ac4749aa6a51996e5493b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e3b5a4125e443b2b4801dc5338ca2ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6850951f313240998daa6c334a0387b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# remove edits for submissions and comments\n",
    "\n",
    "edits_removed_counter = 0\n",
    "\n",
    "# submission texts\n",
    "dataset = dataset.map(lambda x: {\"submission_text\": remove_edits(x[\"submission_text\"])})\n",
    "edits_removal_results['submission_texts']['edits_removed'] = edits_removed_counter\n",
    "\n",
    "# comments\n",
    "for i in range(1, 11):\n",
    "    edits_removed_counter = 0\n",
    "    dataset = dataset.map(lambda x: {f\"top_comment_{i}\": remove_edits(x[f\"top_comment_{i}\"])})\n",
    "    edits_removal_results[f\"top_comment_{i}\"]['edits_removed'] = edits_removed_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add string lengths to result dictionary after removing edits\n",
    "\n",
    "texts_with_potential_edits = {\n",
    "    'submission_texts': dataset[\"submission_text\"],\n",
    "    'top_comment_1': dataset[\"top_comment_1\"],\n",
    "    'top_comment_2': dataset[\"top_comment_2\"],\n",
    "    'top_comment_3': dataset[\"top_comment_3\"],\n",
    "    'top_comment_4': dataset[\"top_comment_4\"],\n",
    "    'top_comment_5': dataset[\"top_comment_5\"],\n",
    "    'top_comment_6': dataset[\"top_comment_6\"],\n",
    "    'top_comment_7': dataset[\"top_comment_7\"],\n",
    "    'top_comment_8': dataset[\"top_comment_8\"],\n",
    "    'top_comment_9': dataset[\"top_comment_9\"],\n",
    "    'top_comment_10': dataset[\"top_comment_10\"],\n",
    "}\n",
    "\n",
    "for key, texts in texts_with_potential_edits.items():\n",
    "    edits_removal_results[key]['avg_length_after_removing_edits'] = get_avg_length(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add percent changes in average lengths from removing edits\n",
    "\n",
    "def calculate_percent_change(before, after):\n",
    "    if before == 0:\n",
    "        return 0\n",
    "    return ((after - before) / before) * 100\n",
    "\n",
    "for key in edits_removal_results.keys():\n",
    "    before = edits_removal_results[key]['avg_length_before_removing_edits']\n",
    "    after = edits_removal_results[key]['avg_length_after_removing_edits']\n",
    "    percent_change = calculate_percent_change(before, after)\n",
    "    edits_removal_results[key]['avg_length_percent_change'] = percent_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save edits removal results\n",
    "\n",
    "output_file = \"processing_results/edits_removal_results.json\"\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(edits_removal_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Upper Extreme Outliers\n",
    "Two-step filtering process:\n",
    "1. Removal of Samples with submissions that are top 5% of flanT5 token count\n",
    "2. Removal of Samples with #1 comments that are top 5% in flanT5 token count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06553fd10d16437fa8b05df30549e304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (583 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5967768973bf49bbb95ce0c9021047fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "def add_token_counts_to_dataset(dataset: Dataset, column: str, tokenizer: PreTrainedTokenizer, new_column_name: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Adds a new column to a specified partition of a dataset with the number of tokens in each row of a specified column.\n",
    "\n",
    "    Parameters:\n",
    "      dataset (Dataset): A Hugging Face dataset object.\n",
    "      column (str): The name of the column in the dataset partition to process.\n",
    "      tokenizer: A Hugging Face transformers pretrained tokenizer\n",
    "      new_column_name (str): The name of the new column to be added to the dataset.\n",
    "\n",
    "    Returns:\n",
    "      Dataset: The modified dataset with an additional column for token counts.\n",
    "    \"\"\"\n",
    "\n",
    "    def count_tokens(row):\n",
    "        row_tokens = tokenizer(row[column], padding=False, truncation=False, return_tensors=\"pt\")\n",
    "        tokens_count = len([tensor.item() for tensor in row_tokens['input_ids'][0]])\n",
    "        return {new_column_name: tokens_count}\n",
    "    \n",
    "    return dataset.map(count_tokens)\n",
    "\n",
    "flanT5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\", trust_remote_code=True)\n",
    "\n",
    "dataset = add_token_counts_to_dataset(dataset, 'submission_text', flanT5_tokenizer, 'submission_text_token_count')\n",
    "dataset = add_token_counts_to_dataset(dataset, 'top_comment_1', flanT5_tokenizer, 'top_comment_1_token_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d8e6bca3dc4403916f7d6d1c03dc25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/80812 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fddf0050ca644d7a8b02490e1d4424b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/76788 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "OUTLIER_PERCENTILE_THRESHOLD = 95\n",
    "\n",
    "def filter_submission_upper_outliers(example):\n",
    "    return example['submission_text_token_count'] <= submission_text_length_threshold\n",
    "\n",
    "def filter_comment_upper_outliers(example):\n",
    "    return example['top_comment_1_token_count'] <= top_comment_length_threshold\n",
    "\n",
    "# Extract lengths into lists\n",
    "submission_text_token_counts = dataset['submission_text_token_count']  \n",
    "top_comment_token_counts = dataset['top_comment_1_token_count']     \n",
    "\n",
    "# Calculate the outlier percentile thresholds\n",
    "submission_text_length_threshold = np.percentile(submission_text_token_counts, OUTLIER_PERCENTILE_THRESHOLD)\n",
    "top_comment_length_threshold = np.percentile(top_comment_token_counts, OUTLIER_PERCENTILE_THRESHOLD)\n",
    "\n",
    "# Filter the dataset\n",
    "rows_before = dataset.num_rows\n",
    "dataset = dataset.filter(filter_submission_upper_outliers)\n",
    "dataset = dataset.filter(filter_comment_upper_outliers)\n",
    "rows_after = dataset.num_rows\n",
    "\n",
    "percent_change = (rows_after - rows_before) / rows_before * 100\n",
    "\n",
    "# Save outlier filtering results\n",
    "outlier_filtering_results ={\n",
    "    \"number of samples before filtering\": rows_before,\n",
    "    \"number of samples after filtering\": rows_after,\n",
    "    \"number of samples removed\": rows_before - rows_after,\n",
    "    \"percent change in number of samples\": percent_change,\n",
    "}\n",
    "\n",
    "output_file = \"processing_results/outlier_filtering_results.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(outlier_filtering_results, f)\n",
    "\n",
    "# Remove the length columns\n",
    "dataset = dataset.remove_columns(['submission_text_token_count', 'top_comment_1_token_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding of Comment AITA Classifications and Ambiguity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def find_earliest_classification(text):\n",
    "    '''\n",
    "    Find the earliest AITA classification in a text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to search for AITA classifications in.\n",
    "\n",
    "    Returns:\n",
    "        str: The earliest classification found in the text.\n",
    "    '''\n",
    "\n",
    "    # classifications mapped to their keywords\n",
    "    classes_dictionary = {\n",
    "      'NTA': ['not the asshole', 'not the a\\*\\*hole', 'nta', 'you would not be the asshole', 'you would not be the a**hole', 'ywnbta', 'n t a', 'y w b t a'],\n",
    "      'NAH': ['no assholes here', 'no a\\*\\*holes here', 'nah', 'n a h'],\n",
    "      'ESH': ['everyone sucks here', 'esh', 'e s h'],\n",
    "      'INFO': ['more information needed', 'more info needed', 'more information required', 'more info required', 'info'],\n",
    "      'YTA': ['you\\'re the asshole', 'you\\'re the a\\*\\*hole', 'youre the asshole', 'youre the a\\*\\*hole', 'yta', 'you would be the asshole', 'you would be the a\\*\\*hole', 'ywbta', 'y t a', 'y w b t a']\n",
    "    }\n",
    "\n",
    "    # track earliest match\n",
    "    earliest_match = None\n",
    "    earliest_match_pos = float('inf')  # Initially set to infinity\n",
    "\n",
    "    # convert input text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # go through all classifications and their keywords\n",
    "    for key, phrases in classes_dictionary.items():\n",
    "        # Create a regex pattern that includes the classification keywords\n",
    "        pattern = r'\\b(' + '|'.join(map(re.escape, phrases)) + r')\\b'\n",
    "\n",
    "        # Search for any keywords in the input text\n",
    "        for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "            if match.start() < earliest_match_pos:\n",
    "                # Update the earliest match if this match is earlier\n",
    "                earliest_match = key\n",
    "                earliest_match_pos = match.start()\n",
    "\n",
    "    # return the class that had the earliest match\n",
    "    return earliest_match\n",
    "\n",
    "def add_classification(row):\n",
    "    '''\n",
    "    Add comment AITA classifications to a row in the datset.\n",
    "\n",
    "    Args:\n",
    "        row (dict): A row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: The row with comment AITA classifications added.\n",
    "    '''\n",
    "    # Iterate over top 10 comment keys\n",
    "    for i in range(1, 11):\n",
    "        key = f'top_comment_{i}'\n",
    "        if key in row and isinstance(row[key], str):\n",
    "            # if this row has a top_comment_N key, get the classification and add it to the row\n",
    "            classification = find_earliest_classification(row[key])\n",
    "            row[key + '_classification'] = classification\n",
    "        else:\n",
    "            # If the top_comment_N key doesn't exist, skip setting this key\n",
    "            row[key + '_classification'] = None\n",
    "\n",
    "    # return the row with the classification added\n",
    "    return row\n",
    "\n",
    "def calculate_ambiguity(classifications):\n",
    "    '''\n",
    "    Calculate the ambiguity score for a list of classifications.\n",
    "\n",
    "    Args:\n",
    "        classifications (list): A list of classifications.\n",
    "\n",
    "    Returns:\n",
    "        float: The ambiguity score.\n",
    "    '''\n",
    "    classification_values = {'YTA': 1, 'ESH': 2,\n",
    "                             'INFO': 3, 'NAH': 4,\n",
    "                             'NTA': 5}\n",
    "\n",
    "    # convert classifications to their numeric representations\n",
    "    numeric_values = [classification_values[c] for c in classifications if c is not None]\n",
    "\n",
    "    # calculate ambiguity score as a function of mean and std dev\n",
    "    mean = np.mean(numeric_values)\n",
    "    std_dev = np.std(numeric_values)\n",
    "    def f(mean):\n",
    "        return (2 - abs(3 - mean)) ** 2 # parabolic that is lowest when mean is 1 or 5 and highest at 3 to emphasize ambiguity for YTA & NTA classes\n",
    "    ambiguity_score = std_dev * f(mean)\n",
    "\n",
    "    # normalize the ambiguity score on a 0-1 scale\n",
    "    min_score = 0  # Minimum possible score (when std dev equals 0)\n",
    "    max_score = 8.0  # Maximum possible score (when classes are equally YTA and NTA which results in max std dev and a central mean)\n",
    "    normalized_score = (ambiguity_score - min_score) / (max_score - min_score)\n",
    "\n",
    "    # return normalized ambiguity score\n",
    "    return normalized_score\n",
    "\n",
    "def add_ambiguity_score(row):\n",
    "    # extract classifications from top comments\n",
    "    classifications = []\n",
    "    for i in range(1, 11):  # Adjust the range based on the number of top comments\n",
    "        classification_key = f'top_comment_{i}_classification'\n",
    "        if classification_key in row and row[classification_key]:\n",
    "            classifications.append(row[classification_key])\n",
    "\n",
    "    # calculate the ambiguity score if there are classifications\n",
    "    if classifications:\n",
    "        row['ambiguity_score'] = calculate_ambiguity(classifications)\n",
    "    else:\n",
    "        row['ambiguity_score'] = None\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac818204271a44a0a4c2393baaed6572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73055 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa4a3b0d8e74b32a2ea690c0e979cc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# add top comment classifications to dataset\n",
    "dataset = dataset.map(add_classification)\n",
    "\n",
    "# convert dataset to dataframe for null filtering\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# remove the rows where the top 1 comment classification is None\n",
    "rows_before = df.shape[0]\n",
    "df_filtered = df[df['top_comment_1_classification'].notnull()]\n",
    "rows_after = df_filtered.shape[0]\n",
    "rows_removed = rows_before - rows_after\n",
    "percent_change = (rows_removed / rows_before) * 100\n",
    "\n",
    "# save results of filtering out rows with null top comment classifications\n",
    "top_comment_classification_null_filtering_results = {\n",
    "    \"number of samples before filtering\": rows_before,\n",
    "    \"number of samples after filtering\": rows_after,\n",
    "    \"number of samples removed\": rows_removed,\n",
    "    \"percent change in number of samples\": percent_change,\n",
    "}\n",
    "\n",
    "output_file = \"processing_results/top_comment_classification_null_filtering_results.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(top_comment_classification_null_filtering_results, f)\n",
    "\n",
    "# convert dataframe back to a dataset\n",
    "dataset = Dataset.from_pandas(df_filtered)\n",
    "\n",
    "# add ambiguity scores to dataset\n",
    "dataset = dataset.map(add_ambiguity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019768cb74524272bf3575d59c324ea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda example: {'AITA_decision': example['decision']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec8fb199b8b4a6eb163ff196032c893",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5d0f5505d7740b89095d815424c3ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Casting the dataset:   0%|          | 0/73048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "# Create a mapping for the unique decision strings to integer labels\n",
    "unique_labels = sorted(set(dataset['decision']))\n",
    "label_to_id = {label: id for id, label in enumerate(unique_labels)}\n",
    "\n",
    "# Define a function to map each decision to its integer label\n",
    "def add_decision_class_label(example):\n",
    "    example['decision_class_label'] = label_to_id[example['decision']]\n",
    "    return example\n",
    "\n",
    "# Add the 'decision_class_label' column to the dataset\n",
    "dataset = dataset.map(add_decision_class_label)\n",
    "\n",
    "# Update the features of the dataset to include 'decision_class_label'\n",
    "new_features = dataset.features.copy()\n",
    "new_features['decision_class_label'] = ClassLabel(names=unique_labels)\n",
    "dataset = dataset.cast(new_features)\n",
    "\n",
    "# Remove the original 'decision' column\n",
    "dataset = dataset.remove_columns('decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af9cdab2bdf347a59f147bbfd0415fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/73048 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def update_aita_decision(sample):\n",
    "    # Check if 'AITA_decision' is 'Asshole' and update it\n",
    "    if sample['AITA_decision'] == 'Asshole':\n",
    "        sample['AITA_decision'] = 'A-hole'\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(update_aita_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(\n",
    "    test_size = 0.2,\n",
    "    stratify_by_column='decision_class_label',\n",
    "    seed=42 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding of Flan-T5 and Llama-2 Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28cebb44862844b691b12882c31e4623",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58438 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2aa0b44ea0dd4575b3edac2ffa7cef59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from AITA_instruction import AITA_Instruction\n",
    "\n",
    "# adding of Flan-T5 instructions\n",
    "dataset[\"train\"] = dataset[\"train\"].map(AITA_Instruction.get_flanT5_instruction, batched=False)\n",
    "dataset[\"test\"] = dataset[\"test\"].map(AITA_Instruction.get_flanT5_instruction, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6741aee1770a41149ba68b9d7791084a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/58438 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41df0469082b43c08fcfe6ddde898957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14610 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# adding of Llama2 instructions\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    lambda sample: AITA_Instruction.get_llama2_training_instruction(sample, instruction_type=\"training\"), \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "dataset[\"test\"] = dataset[\"test\"].map(\n",
    "    lambda sample: AITA_Instruction.get_llama2_training_instruction(sample, instruction_type=\"testing\"), \n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].remove_columns([\"__index_level_0__\", \"decision_class_label\"])\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns([\"__index_level_0__\", \"decision_class_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Sort the 'train' subset by 'submission_score' and select the top 40000 rows\n",
    "sorted_train = dataset['train'].sort('submission_score', reverse=True)\n",
    "top_train = sorted_train.select(range(40000))\n",
    "\n",
    "# Sort the 'test' subset by 'submission_score' and select the top 10000 rows\n",
    "sorted_test = dataset['test'].sort('submission_score', reverse=True)\n",
    "top_test = sorted_test.select(range(10000))\n",
    "\n",
    "shuffled_train = top_train.shuffle(seed=42)\n",
    "shuffled_test = top_test.shuffle(seed=42)\n",
    "\n",
    "# Create a new DatasetDict with the filtered data\n",
    "dataset = DatasetDict({\"train\": top_train, \"test\": top_test})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving of Top 50k dataset to HuggingFace Hub\n",
    "- by submission score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d874610e8ccc4213a3bbf3f159ca4b3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72f61395b8574450ac241e1530cd0bdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/40 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f2bd2d48f384c96b3e151f95381906b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34cbbfdc42004410b522399ab78016a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/MattBoraske/reddit-AITA-submissions-and-comments-top-50k/commit/8ac2e1681b904ada0febec77549fd04b2d951624', commit_message='Upload dataset', commit_description='', oid='8ac2e1681b904ada0febec77549fd04b2d951624', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Sort the 'train' subset by 'submission_score' and select the top 40000 rows\n",
    "sorted_train = dataset['train'].sort('submission_score', reverse=True)\n",
    "top_train = sorted_train.select(range(40000))\n",
    "\n",
    "# Sort the 'test' subset by 'submission_score' and select the top 10000 rows\n",
    "sorted_test = dataset['test'].sort('submission_score', reverse=True)\n",
    "top_test = sorted_test.select(range(10000))\n",
    "\n",
    "shuffled_train = top_train.shuffle(seed=42)\n",
    "shuffled_test = top_test.shuffle(seed=42)\n",
    "\n",
    "# Create a new DatasetDict with the filtered data\n",
    "dataset = DatasetDict({\"train\": top_train, \"test\": top_test})\n",
    "\n",
    "dataset.push_to_hub(f'reddit-AITA-submissions-and-comments-top-50k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving of Top 2k Dataset to HuggingFace Hub\n",
    "- by submission score\n",
    "    - Top 320/80 training/testing for each AITA classification\n",
    "- 4% of the size of the main 50k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def filter_top_samples_df(dataset, top_n):\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Group by 'decision_class_label', sort within groups by 'submission_score', and take top N\n",
    "    grouped = df.groupby('AITA_decision', group_keys=False).apply(lambda x: x.nlargest(top_n, 'submission_score'))\n",
    "    return Dataset.from_pandas(grouped)\n",
    "\n",
    "# Filter the datasets and convert to DataFrames\n",
    "filtered_train_df = filter_top_samples_df(dataset['train'], 320)\n",
    "filtered_test_df = filter_top_samples_df(dataset['test'], 80)\n",
    "\n",
    "# Create a new DatasetDict\n",
    "samples_2000_dataset = DatasetDict({\n",
    "    'train': filtered_train_df,\n",
    "    'test': filtered_test_df\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_2000_dataset[\"train\"] = samples_2000_dataset[\"train\"].remove_columns([\"__index_level_0__\"])\n",
    "samples_2000_dataset[\"test\"] = samples_2000_dataset[\"test\"].remove_columns([\"__index_level_0__\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c2a93916a1454dacd153dd75512d14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd2dfaa7f71a4d31bce80b93f44e94dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb839de5b65d449881fbb3ec4804e25d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0edb8ed925d42eda12f063b3c52f9e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/MattBoraske/reddit-AITA-submissions-and-comments-top-2k/commit/31163fe2ea8970f8f5fbebdfe31b0003f8932957', commit_message='Upload dataset', commit_description='', oid='31163fe2ea8970f8f5fbebdfe31b0003f8932957', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples_2000_dataset.push_to_hub('reddit-AITA-submissions-and-comments-top-2k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
