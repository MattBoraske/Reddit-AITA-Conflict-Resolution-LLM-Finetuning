{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff that belongs in another notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ambiguity score analysis for both main and high-quality datasets\n",
    "- measures of agreement and for both main and high quality datasets\n",
    "- token histograms for main and high quality datasets (regular ones for flan-t5, llama2_text ones for llama2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir results\\Decision_Ambiguity_Analysis\n",
    "!mkdir results\\Comment_Agreement_Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histogram Plotting Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def plot_histogram(data, plot_title, plot_xlabel, plot_ylabel, plot_file, bin_counts_file, log_y=False):\n",
    "  '''\n",
    "  Plots a histogram of data\n",
    "\n",
    "  Parameters:\n",
    "    data (numpy array): The data to plot.\n",
    "    plot_title (str): The title of the plot.\n",
    "    plot_xlabel (str): The label for the x-axis.\n",
    "    plot_ylabel (str): The label for the y-axis.\n",
    "    plot_file (str): The file path to save the plot.\n",
    "    bin_counts_file (str): The file path to save the bin counts.\n",
    "    log_y (bool): Whether to set the y-axis to logarithmic scale.\n",
    "\n",
    "  Returns:\n",
    "    None - The plot and bin counts are saved to the specified files.\n",
    "  '''\n",
    "\n",
    "  # Calculate the common bin edges for both train and test submissions\n",
    "  bin_edges = np.histogram_bin_edges(data, bins=20)\n",
    "\n",
    "  # Get histogram bin counts for train and test submissions\n",
    "  counts, _ = np.histogram(data, bins=bin_edges)\n",
    "\n",
    "  # Plot histograms\n",
    "  plt.figure(figsize=(12, 6))\n",
    "  sns.histplot(data, bins=bin_edges, kde=True, color='blue')\n",
    "  plt.xlabel(plot_xlabel, fontsize=14)\n",
    "  plt.ylabel(plot_ylabel, fontsize=14)\n",
    "  \n",
    "  if log_y == True:\n",
    "    plt.yscale('log')  # Set y-axis to logarithmic scale\n",
    "\n",
    "  plt.title(plot_title, fontsize=16)\n",
    "  plt.savefig(plot_file)\n",
    "  plt.show()\n",
    "\n",
    "  # Log Histogram Bin Counts\n",
    "  bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "  counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Counts': counts}, index=bin_labels)\n",
    "\n",
    "  # Pretty Print counts dataframe\n",
    "  print(counts_df.to_string(index=False))\n",
    "\n",
    "  # Save counts dataframe to CSV\n",
    "  counts_df.to_csv(bin_counts_file, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Token Analysis Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "def get_token_counts(dataset: dict, partition: str, column: str, tokenizer: PreTrainedTokenizer) -> list:\n",
    "    \"\"\"\n",
    "    Counts the number of tokens in each row of a specified column in a dataset partition using a provided tokenizer.\n",
    "\n",
    "    This function iterates through rows of a specified column in a given dataset partition,\n",
    "    tokenizes each row using the provided tokenizer, and counts the number of tokens\n",
    "    generated for each row.\n",
    "\n",
    "    Parameters:\n",
    "      dataset (dict): A huggingface dataset object.\n",
    "      partition (str): The specific partition of the dataset to analyze (e.g., 'train').\n",
    "      column (str): The name of the column in the dataset partition to process.\n",
    "      tokenizer: A huggingface transformers pretrained tokenizer\n",
    "\n",
    "    Returns:\n",
    "      tokens_counts: A list of integers, where each integer represents the number of tokens in the\n",
    "      corresponding row of the specified column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract the data from the specified column in the given partition of the dataset\n",
    "    column_data = dataset[partition][column]\n",
    "\n",
    "    # Initialize an empty list to store token counts for each row\n",
    "    tokens_counts = []\n",
    "\n",
    "    # Iterate through each row in the column data\n",
    "    for row in column_data:\n",
    "        # Tokenize the row and count the number of tokens\n",
    "        row_tokens = tokenizer(row, padding=False, truncation=False, return_tensors=\"pt\")\n",
    "        tokens_count = len([tensor.item() for tensor in row_tokens['input_ids'][0]])\n",
    "\n",
    "        # Append the token count to the list\n",
    "        tokens_counts.append(tokens_count)\n",
    "\n",
    "    # Return the list of token counts\n",
    "    return tokens_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "def add_token_counts_to_dataset(dataset: Dataset, partition: str, column: str, tokenizer: PreTrainedTokenizer, new_column_name: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Adds a new column to a specified partition of a dataset with the number of tokens in each row of a specified column.\n",
    "\n",
    "    Parameters:\n",
    "      dataset (Dataset): A Hugging Face dataset object.\n",
    "      partition (str): The specific partition of the dataset to analyze (e.g., 'train').\n",
    "      column (str): The name of the column in the dataset partition to process.\n",
    "      tokenizer: A Hugging Face transformers pretrained tokenizer\n",
    "      new_column_name (str): The name of the new column to be added to the dataset.\n",
    "\n",
    "    Returns:\n",
    "      Dataset: The modified dataset with an additional column for token counts.\n",
    "    \"\"\"\n",
    " \n",
    "    def count_tokens(row):\n",
    "        # Tokenize the text and count the number of tokens\n",
    "        row_tokens = tokenizer(row[column], padding=False, truncation=False, return_tensors=\"pt\")\n",
    "        tokens_count = len([tensor.item() for tensor in row_tokens['input_ids'][0]])\n",
    "        return {new_column_name: tokens_count}\n",
    "\n",
    "    # Update the specified partition of the dataset\n",
    "    updated_dataset = dataset[partition].map(count_tokens)\n",
    "\n",
    "    return updated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "def filter_by_token_count(dataset: Dataset, max_tokens: int, token_count_column: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Filters out rows in a dataset where the token count exceeds a specified maximum.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (Dataset): A Hugging Face dataset object.\n",
    "    max_tokens (int): The maximum allowed number of tokens.\n",
    "    token_count_column (str): The name of the column containing the token counts. Default is 'token_count'.\n",
    "\n",
    "    Returns:\n",
    "    Dataset: A new dataset with rows filtered based on the token count criteria.\n",
    "    \"\"\"\n",
    "\n",
    "    def is_within_max_tokens(row):\n",
    "        # Check if the token count for the row is less than or equal to max_tokens\n",
    "        return row[token_count_column] <= max_tokens\n",
    "\n",
    "    # Filter the dataset\n",
    "    filtered_dataset = dataset.filter(is_within_max_tokens)\n",
    "\n",
    "    return filtered_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ambiguity Score Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot token counts on log y histogram then save it and the bin counts\n",
    "\n",
    "TITLE = \"Ambiguity Scores for Reddit AITA Dataset\"\n",
    "\n",
    "plot_histogram(\n",
    "    data = dataset[\"ambiguity_score\"],\n",
    "    plot_title = \"Ambiguity Scores for Reddit AITA Dataset\",\n",
    "    plot_xlabel = \"Score\",\n",
    "    plot_ylabel = \"Frequency\",\n",
    "    plot_file = f\"results/Decision_Ambiguity_Analysis/Ambiguity Scores.png\",\n",
    "    bin_counts_file = f\"results/Decision_Ambiguity_Analysis/Ambiguity Scores.csv\",\n",
    "    log_y = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# calculation and plotting of ambiguity score percentiles\n",
    "\n",
    "percentiles = np.percentile(dataset[\"ambiguity_score\"], np.arange(0, 101, 1))\n",
    "percentile_data = pd.DataFrame({\n",
    "    \"Percentile\": np.arange(0, 101, 1),\n",
    "    \"Ambiguity Score\": percentiles\n",
    "})\n",
    "percentile_data.to_csv(\"results/Decision_Ambiguity_Analysis/Ambiguity Score Percentiles.csv\", index=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))  # You can adjust the figure size as needed\n",
    "plt.plot(percentiles, np.arange(0, 101, 1))\n",
    "plt.xlabel(\"Ambiguity Score\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Ambiguity Score Percentiles for Reddit AITA Dataset\")\n",
    "plt.savefig(\"results/Decision_Ambiguity_Analysis/Ambiguity Score Percentiles.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the dataset to include only samples with an ambiguity score of 0\n",
    "zero_ambiguity_dataset = dataset.filter(lambda x: x['ambiguity_score'] == 0)\n",
    "\n",
    "# Get counts for each dataset split\n",
    "zero_ambiguity_count = len(zero_ambiguity_dataset)\n",
    "\n",
    "\n",
    "# Calculate percentages\n",
    "total_count= len(dataset)\n",
    "zero_ambiguity_percentage = round((zero_ambiguity_count / total_count) * 100, 3)\n",
    "\n",
    "# Store results in dataframe and save to output CSV\n",
    "zero_ambiguity_results = {\n",
    "    'Number of Samples with Zero Ambiguity': [zero_ambiguity_count],\n",
    "    'Percentage of Samples with Zero Ambiguity': [zero_ambiguity_percentage]\n",
    "}\n",
    "\n",
    "output_file = \"results/Decision_Ambiguity_Analysis/zero_ambiguity_samples_results.json\"\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(zero_ambiguity_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Agreement Between Top Comments\n",
    "- Overall: Krippendorff's Alpha\n",
    "  - Key aspects\n",
    "    - \"Krippendorff's alpha coefficient,[1] named after academic Klaus Krippendorff, is a statistical measure of the agreement achieved when coding a set of units of analysis. Since the 1970s, alpha has been used in content analysis where textual units are categorized by trained readers, in counseling and survey research where experts code open-ended interview data into analyzable terms, in psychological testing where alternative tests of the same phenomena need to be compared, or in observational studies where unstructured happenings are recorded for subsequent analysis.\"\n",
    "    - \"Krippendorff's alpha generalizes several known statistics, often called measures of inter-coder agreement, inter-rater reliability, reliability of coding given sets of units (as distinct from unitizing) but it also distinguishes itself from statistics that are called reliability coefficients but are unsuitable to the particulars of coding data generated for subsequent analysis.\"\n",
    "    - \"Krippendorff's alpha is applicable to any number of coders, each assigning one value to one unit of analysis, to incomplete (missing) data, to any number of values available for coding a variable, to binary, nominal, ordinal, interval, ratio, polar, and circular metrics (note that this is not a metric in the mathematical sense, but often the square of a mathematical metric, see levels of measurement), and it adjusts itself to small sample sizes of the reliability data. The virtue of a single coefficient with these variations is that computed reliabilities are comparable across any numbers of coders, values, different metrics, and unequal sample sizes.\n",
    "  - [Wiki](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha)\n",
    "  - [Lecture by Krippendorff on Calculation](https://www.asc.upenn.edu/sites/default/files/2021-03/Computing%20Krippendorff%27s%20Alpha-Reliability.pdf)\n",
    "  - [Article Explanation](https://www.surgehq.ai/blog/inter-rater-reliability-metrics-an-introduction-to-krippendorffs-alpha)\n",
    "    - Ranges from -1 to 1, with -1 being complete disagreement, 0 being random choice, and 1 being complete agreement\n",
    "    - 0.8 indicates significant agreement.\n",
    "\n",
    "- Pairwise: Cohen's Kappa\n",
    "  - [Wiki](https://en.wikipedia.org/wiki/Cohen%27s_kappa)\n",
    "  - [Article Explanation](https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import krippendorff\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from itertools import combinations\n",
    "\n",
    "def get_encoded_classifications(dataset):\n",
    "    \"\"\"\n",
    "    Encodes AITA classifications into numeric values, retaining None values.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (list of dictionaries): A huggingface dataset\n",
    "\n",
    "    Returns:\n",
    "    list[list]: Lists of numeric classifications, with None where input was None\n",
    "    \"\"\"\n",
    "\n",
    "    # Mapping of AITA classifications to numeric values\n",
    "    classification_values = {'YTA': 1, 'ESH': 2,\n",
    "                             'INFO': 3, 'NAH': 4,\n",
    "                             'NTA': 5}\n",
    "\n",
    "    # Initialize a list of lists, one for each of the top 10 comments\n",
    "    top_comments = [[] for _ in range(10)]\n",
    "\n",
    "    # Iterate over each sample in the dataset\n",
    "    for sample in dataset:\n",
    "        # Iterate over the top 10 comments\n",
    "        for i in range(10):\n",
    "            key = f'top_comment_{i+1}_classification'\n",
    "            # Append the classification to the corresponding list\n",
    "            top_comments[i].append(sample.get(key, None))\n",
    "\n",
    "    # Convert classifications to their numeric representations, keeping None as is\n",
    "    top_comments_encoded = []\n",
    "    for i in range(len(top_comments)):\n",
    "        encoded_comment = [classification_values.get(c, None) for c in top_comments[i]]\n",
    "        top_comments_encoded.append(encoded_comment)\n",
    "    return top_comments_encoded\n",
    "\n",
    "\n",
    "def calculate_krippendorffs_alpha(dataset):\n",
    "  \"\"\"\n",
    "  Calculates Krippendorff's alpha for a given dataset.\n",
    "\n",
    "  Parameters:\n",
    "  dataset (list of dictionaries): A huggingface dataset.\n",
    "\n",
    "  Returns:\n",
    "  float: Krippendorff's alpha score.\n",
    "  \"\"\"\n",
    "\n",
    "  # Encode top comment classifications\n",
    "  top_comments_encoded = get_encoded_classifications(dataset)\n",
    "\n",
    "  # Calculate and return krippendorff's alpha\n",
    "  data = np.array([[np.nan if x is None else x for x in sublist] for sublist in top_comments_encoded], dtype=float)\n",
    "  return krippendorff.alpha(data)\n",
    "\n",
    "\n",
    "def calculate_cohen_kappa(dataset):\n",
    "  \"\"\"\n",
    "  Calculates Cohen's Kappa score for a given dataset.\n",
    "\n",
    "  Parameters:\n",
    "  dataset (list of dictionaries): A huggingface dataset.\n",
    "\n",
    "  Returns:\n",
    "  dict: A dictionary of Cohen's Kappa scores for each pair of top comments.\n",
    "  \"\"\"\n",
    "\n",
    "  # encode top comment classifications\n",
    "  top_comments_encoded = get_encoded_classifications(dataset)\n",
    "\n",
    "  scores = {}\n",
    "  for list1, list2 in combinations(top_comments_encoded, 2):\n",
    "      filtered_list1 = []\n",
    "      filtered_list2 = []\n",
    "      for true, pred in zip(list1, list2):\n",
    "          if true is not None and pred is not None:\n",
    "              filtered_list1.append(true)\n",
    "              filtered_list2.append(pred)\n",
    "      score = cohen_kappa_score(filtered_list1, filtered_list2)\n",
    "      index1 = top_comments_encoded.index(list1)\n",
    "      index2 = top_comments_encoded.index(list2)\n",
    "      key = (f\"top_comment_{index1 + 1}\", f\"top_comment_{index2 + 1}\")\n",
    "      scores[key] = score\n",
    "  return scores\n",
    "\n",
    "\n",
    "def save_cohen_kappa_scores(cohen_kappa_scores, output_file):\n",
    "  \"\"\"\n",
    "  Saves Cohen's Kappa scores to a CSV file.\n",
    "\n",
    "  Parameters:\n",
    "  cohen_kappa_scores (dict): A dictionary of Cohen's Kappa scores.\n",
    "  output_file (str): The path to the output CSV file.\n",
    "  \"\"\"\n",
    "\n",
    "  # create a list of column and row names\n",
    "  comments = [f\"top_comment_{i}\" for i in range(1, 11)]\n",
    "\n",
    "  # create an empty dataframe and fill with scores\n",
    "  df = pd.DataFrame(index=comments, columns=comments)\n",
    "  for (comment1, comment2), score in cohen_kappa_scores.items():\n",
    "      df.at[comment1, comment2] = round(score, 3)\n",
    "\n",
    "  # set the lower triangle to NaN, including the diagonal\n",
    "  for i in range(len(df)):\n",
    "      for j in range(i + 1):\n",
    "          df.iat[i, j] = np.nan\n",
    "\n",
    "  # save the dataframe\n",
    "  df.to_csv(output_file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# calculate Krippendorrf's alphas for train and test datasets then save them to JSON\n",
    "krippendorffs_alpha = {'krippendorffs alpha': calculate_krippendorffs_alpha(dataset)}\n",
    "\n",
    "output_file = \"results/Comment_Agreement_Analysis/krippendorffs_alpha.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(krippendorffs_alpha, f)\n",
    "\n",
    "# calculate Cohen's Kappa scores for train and test datasets and save them to CSV\n",
    "cohen_kappa_scores = calculate_cohen_kappa(dataset)\n",
    "cohen_kappa_scores = {str(key): value for key, value in cohen_kappa_scores.items()}\n",
    "\n",
    "output_file = \"results/Comment_Agreement_Analysis/cohen_kappa_scores.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(cohen_kappa_scores, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
