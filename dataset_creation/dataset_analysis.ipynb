{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit AITA Dataset Analysis\n",
    "\n",
    "Datasets Analyzed:\n",
    "- [Reddit AITA Multiclass](https://huggingface.co/datasets/MattBoraske/reddit-AITA-submissions-and-comments-multiclass)\n",
    "- [Reddit AITA Multiclass Top 2k](https://huggingface.co/datasets/MattBoraske/reddit-AITA-submissions-and-comments-multiclass-top-2k)\n",
    "- [Reddit AITA Binary](https://huggingface.co/datasets/MattBoraske/reddit-AITA-submissions-and-comments-binary)\n",
    "- [Reddit AITA Binary Top 2k](https://huggingface.co/datasets/MattBoraske/reddit-AITA-submissions-and-comments-binary-top-2k)\n",
    "\n",
    "Analysis Done:\n",
    "- Decision Ambiguity Analysis - Scores Histogram, Cumulative Freqency Plot, Zero Ambiguity Proportion\n",
    "- Comment Agreement Analysis - Krippendorff's Alpha (holistic) and Cohen's Kappa (pairwise)\n",
    "- Flan-T5 and Llama-2 Token Count Analysis - Histograms that show the distribution of tokens for each prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers krippendorff seaborn huggingface_hub ipywidgets pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir analysis_results\\multiclass\\Decision_Ambiguity_Analysis\n",
    "!mkdir analysis_results\\multiclass\\Comment_Agreement_Analysis\n",
    "!mkdir analysis_results\\multiclass\\Token_Count_Analysis\n",
    "\n",
    "!mkdir analysis_results\\multiclass-top-2k\\Decision_Ambiguity_Analysis\n",
    "!mkdir analysis_results\\multiclass-top-2k\\Comment_Agreement_Analysis\n",
    "!mkdir analysis_results\\multiclass-top-2k\\Token_Count_Analysis\n",
    "\n",
    "!mkdir analysis_results\\binary\\Decision_Ambiguity_Analysis\n",
    "!mkdir analysis_results\\binary\\Comment_Agreement_Analysis\n",
    "!mkdir analysis_results\\binary\\Token_Count_Analysis\n",
    "\n",
    "!mkdir analysis_results\\binary-top-2k\\Decision_Ambiguity_Analysis\n",
    "!mkdir analysis_results\\binary-top-2k\\Comment_Agreement_Analysis\n",
    "!mkdir analysis_results\\binary-top-2k\\Token_Count_Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "multiclass_dataset = load_dataset('MattBoraske/reddit-AITA-submissions-and-comments-multiclass')\n",
    "multiclass_top_2k_dataset = load_dataset('MattBoraske/reddit-AITA-submissions-and-comments-multiclass-top-2k')\n",
    "binary_dataset = load_dataset('MattBoraske/reddit-AITA-submissions-and-comments-binary')\n",
    "binary_top_2k_dataset = load_dataset('MattBoraske/reddit-AITA-submissions-and-comments-binary-top-2k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flan-T5 and Llama-2 Token Count Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "def add_token_counts_to_dataset(dataset: Dataset, column: str, tokenizer: PreTrainedTokenizer, new_column_name: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Adds a new column to a specified partition of a dataset with the number of tokens in each row of a specified column.\n",
    "\n",
    "    Parameters:\n",
    "      dataset (Dataset): A Hugging Face dataset object.\n",
    "      column (str): The name of the column in the dataset partition to process.\n",
    "      tokenizer: A Hugging Face transformers pretrained tokenizer\n",
    "      new_column_name (str): The name of the new column to be added to the dataset.\n",
    "\n",
    "    Returns:\n",
    "      Dataset: The modified dataset with an additional column for token counts.\n",
    "    \"\"\"\n",
    "\n",
    "    def count_tokens(row):\n",
    "        row_tokens = tokenizer(row[column], padding=False, truncation=False, return_tensors=\"pt\")\n",
    "        tokens_count = len([tensor.item() for tensor in row_tokens['input_ids'][0]])\n",
    "        return {new_column_name: tokens_count}\n",
    "    \n",
    "    return dataset.map(count_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flanT5_tokenizer = AutoTokenizer.from_pretrained(\"MattBoraske/flan-t5-xl-reddit-AITA-multiclass\", trust_remote_code=True)\n",
    "llama2_tokenizer = AutoTokenizer.from_pretrained(\"MattBoraske/llama-2-7b-chat-reddit-AITA-multiclass\", trust_remote_code=True)\n",
    "\n",
    "multiclass_dataset = add_token_counts_to_dataset(multiclass_dataset, 'flanT5_instruction', flanT5_tokenizer, 'flanT5_instruction_token_count')\n",
    "multiclass_dataset = add_token_counts_to_dataset(multiclass_dataset, 'llama2_instruction', llama2_tokenizer, 'llama2_instruction_token_count')\n",
    "\n",
    "multiclass_top_2k_dataset = add_token_counts_to_dataset(multiclass_top_2k_dataset, 'flanT5_instruction', flanT5_tokenizer, 'flanT5_instruction_token_count')\n",
    "multiclass_top_2k_dataset = add_token_counts_to_dataset(multiclass_top_2k_dataset, 'llama2_instruction', llama2_tokenizer, 'llama2_instruction_token_count')\n",
    "\n",
    "binary_dataset = add_token_counts_to_dataset(binary_dataset, 'flanT5_instruction', flanT5_tokenizer, 'flanT5_instruction_token_count')\n",
    "binary_dataset = add_token_counts_to_dataset(binary_dataset, 'llama2_instruction', llama2_tokenizer, 'llama2_instruction_token_count')\n",
    "\n",
    "binary_top_2k_dataset = add_token_counts_to_dataset(binary_top_2k_dataset, 'flanT5_instruction', flanT5_tokenizer, 'flanT5_instruction_token_count')\n",
    "binary_top_2k_dataset = add_token_counts_to_dataset(binary_top_2k_dataset, 'llama2_instruction', llama2_tokenizer, 'llama2_instruction_token_count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_token_counts(dataset, split):\n",
    "    token_counts = {\n",
    "        'llama2_instruction_token_count': [],\n",
    "        'flanT5_instruction_token_count': []\n",
    "    }\n",
    "\n",
    "    # Iterate over all examples in the split\n",
    "    for example in dataset[split]:\n",
    "        token_counts['llama2_instruction_token_count'].append(example['llama2_instruction_token_count'])\n",
    "        token_counts['flanT5_instruction_token_count'].append(example['flanT5_instruction_token_count'])\n",
    "\n",
    "    return token_counts\n",
    "\n",
    "# Get train/test token counts for each dataset\n",
    "multiclass_token_counts = {\n",
    "    'train': calculate_token_counts(multiclass_dataset, 'train'),\n",
    "    'test': calculate_token_counts(multiclass_dataset, 'test')\n",
    "}\n",
    "\n",
    "multiclass_2k_token_counts = {\n",
    "    'train': calculate_token_counts(multiclass_top_2k_dataset, 'train'),\n",
    "    'test': calculate_token_counts(multiclass_top_2k_dataset, 'test')\n",
    "}\n",
    "\n",
    "binary_token_counts = {\n",
    "    'train': calculate_token_counts(binary_dataset, 'train'),\n",
    "    'test': calculate_token_counts(binary_dataset, 'test')\n",
    "}\n",
    "\n",
    "binary_2k_token_counts = {\n",
    "    'train': calculate_token_counts(binary_top_2k_dataset, 'train'),\n",
    "    'test': calculate_token_counts(binary_top_2k_dataset, 'test')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([multiclass_token_counts['train']['flanT5_instruction_token_count'], multiclass_token_counts['test']['flanT5_instruction_token_count']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "flant5_train_counts, _ = np.histogram(multiclass_token_counts['train']['flanT5_instruction_token_count'], bins=bin_edges)\n",
    "flant5_test_counts, _ = np.histogram(multiclass_token_counts['test']['flanT5_instruction_token_count'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(multiclass_token_counts['train']['flanT5_instruction_token_count'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(multiclass_token_counts['test']['flanT5_instruction_token_count'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Multiclass Flan-T5 Tokenized Prompt Lengths', fontsize=16)\n",
    "plt.xlabel('Number of Tokens', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/multiclass/Token_Count_Analysis/flanT5_tokenized_lengths.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "flant5_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': flant5_train_counts, 'Train': flant5_test_counts}, index=bin_labels)\n",
    "flant5_counts_df.to_csv('analysis_results/multiclass/Token_Count_Analysis/flanT5_tokenized_lengths_bins.csv', index=False)\n",
    "\n",
    "\n",
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([multiclass_token_counts['train']['llama2_instruction_token_count'], multiclass_token_counts['test']['llama2_instruction_token_count']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "ambiguity_train_counts, _ = np.histogram(multiclass_token_counts['train']['llama2_instruction_token_count'], bins=bin_edges)\n",
    "ambiguity_test_counts, _ = np.histogram(multiclass_token_counts['test']['llama2_instruction_token_count'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(multiclass_token_counts['train']['llama2_instruction_token_count'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(multiclass_token_counts['test']['llama2_instruction_token_count'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Multiclass Llama 2 Tokenized Prompt Lengths', fontsize=16)\n",
    "plt.xlabel('Number of Tokens', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/multiclass/Token_Count_Analysis/llama2_tokenized_lengths.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "llama2_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': ambiguity_train_counts, 'Train': ambiguity_test_counts}, index=bin_labels)\n",
    "llama2_counts_df.to_csv('analysis_results/multiclass/Token_Count_Analysis/llama2_tokenized_lengths_bins.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([multiclass_2k_token_counts['train']['flanT5_instruction_token_count'], multiclass_2k_token_counts['test']['flanT5_instruction_token_count']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "flant5_train_counts, _ = np.histogram(multiclass_2k_token_counts['train']['flanT5_instruction_token_count'], bins=bin_edges)\n",
    "flant5_test_counts, _ = np.histogram(multiclass_2k_token_counts['test']['flanT5_instruction_token_count'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(multiclass_2k_token_counts['train']['flanT5_instruction_token_count'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(multiclass_2k_token_counts['test']['flanT5_instruction_token_count'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Multiclass-Top-2k Flan-T5 Tokenized Prompt Lengths', fontsize=16)\n",
    "plt.xlabel('Number of Tokens', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/multiclass-top-2k/Token_Count_Analysis/flanT5_tokenized_lengths.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "flant5_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': flant5_train_counts, 'Train': flant5_test_counts}, index=bin_labels)\n",
    "flant5_counts_df.to_csv('analysis_results/multiclass-top-2k/Token_Count_Analysis/flanT5_tokenized_lengths_bins.csv', index=False)\n",
    "\n",
    "\n",
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([multiclass_2k_token_counts['train']['llama2_instruction_token_count'], multiclass_2k_token_counts['test']['llama2_instruction_token_count']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "ambiguity_train_counts, _ = np.histogram(multiclass_2k_token_counts['train']['llama2_instruction_token_count'], bins=bin_edges)\n",
    "ambiguity_test_counts, _ = np.histogram(multiclass_2k_token_counts['test']['llama2_instruction_token_count'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(multiclass_2k_token_counts['train']['llama2_instruction_token_count'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(multiclass_2k_token_counts['test']['llama2_instruction_token_count'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Multiclass-Top-2k Llama 2 Tokenized Prompt Lengths', fontsize=16)\n",
    "plt.xlabel('Number of Tokens', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/multiclass-top-2k/Token_Count_Analysis/llama2_tokenized_lengths.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "llama2_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': ambiguity_train_counts, 'Train': ambiguity_test_counts}, index=bin_labels)\n",
    "llama2_counts_df.to_csv('analysis_results/multiclass-top-2k/Token_Count_Analysis/llama2_tokenized_lengths_bins.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([binary_token_counts['train']['flanT5_instruction_token_count'], binary_token_counts['test']['flanT5_instruction_token_count']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "flant5_train_counts, _ = np.histogram(binary_token_counts['train']['flanT5_instruction_token_count'], bins=bin_edges)\n",
    "flant5_test_counts, _ = np.histogram(binary_token_counts['test']['flanT5_instruction_token_count'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(binary_token_counts['train']['flanT5_instruction_token_count'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(binary_token_counts['test']['flanT5_instruction_token_count'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Binary Flan-T5 Tokenized Prompt Lengths', fontsize=16)\n",
    "plt.xlabel('Number of Tokens', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/binary/Token_Count_Analysis/flanT5_tokenized_lengths.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "flant5_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': flant5_train_counts, 'Train': flant5_test_counts}, index=bin_labels)\n",
    "flant5_counts_df.to_csv('analysis_results/binary/Token_Count_Analysis/flanT5_tokenized_lengths_bins.csv', index=False)\n",
    "\n",
    "\n",
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([binary_token_counts['train']['llama2_instruction_token_count'], binary_token_counts['test']['llama2_instruction_token_count']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "ambiguity_train_counts, _ = np.histogram(binary_token_counts['train']['llama2_instruction_token_count'], bins=bin_edges)\n",
    "ambiguity_test_counts, _ = np.histogram(binary_token_counts['test']['llama2_instruction_token_count'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(binary_token_counts['train']['llama2_instruction_token_count'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(binary_token_counts['test']['llama2_instruction_token_count'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Binary Llama 2 Tokenized Prompt Lengths', fontsize=16)\n",
    "plt.xlabel('Number of Tokens', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/binary/Token_Count_Analysis/llama2_tokenized_lengths.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "llama2_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': ambiguity_train_counts, 'Train': ambiguity_test_counts}, index=bin_labels)\n",
    "llama2_counts_df.to_csv('analysis_results/binary/Token_Count_Analysis/llama2_tokenized_lengths_bins.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([binary_2k_token_counts['train']['flanT5_instruction_token_count'], binary_2k_token_counts['test']['flanT5_instruction_token_count']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "flant5_train_counts, _ = np.histogram(binary_2k_token_counts['train']['flanT5_instruction_token_count'], bins=bin_edges)\n",
    "flant5_test_counts, _ = np.histogram(binary_2k_token_counts['test']['flanT5_instruction_token_count'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(binary_2k_token_counts['train']['flanT5_instruction_token_count'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(binary_2k_token_counts['test']['flanT5_instruction_token_count'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Binary-Top-2k Flan-T5 Tokenized Prompt Lengths', fontsize=16)\n",
    "plt.xlabel('Number of Tokens', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/binary-top-2k/Token_Count_Analysis/flanT5_tokenized_lengths.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "flant5_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': flant5_train_counts, 'Train': flant5_test_counts}, index=bin_labels)\n",
    "flant5_counts_df.to_csv('analysis_results/binary-top-2k/Token_Count_Analysis/flanT5_tokenized_lengths_bins.csv', index=False)\n",
    "\n",
    "\n",
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([binary_2k_token_counts['train']['llama2_instruction_token_count'], binary_2k_token_counts['test']['llama2_instruction_token_count']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "ambiguity_train_counts, _ = np.histogram(binary_2k_token_counts['train']['llama2_instruction_token_count'], bins=bin_edges)\n",
    "ambiguity_test_counts, _ = np.histogram(binary_2k_token_counts['test']['llama2_instruction_token_count'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(binary_2k_token_counts['train']['llama2_instruction_token_count'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(binary_2k_token_counts['test']['llama2_instruction_token_count'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Binary-Top-2k Llama 2 Tokenized Prompt Lengths', fontsize=16)\n",
    "plt.xlabel('Number of Tokens', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/binary-top-2k/Token_Count_Analysis/llama2_tokenized_lengths.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "llama2_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': ambiguity_train_counts, 'Train': ambiguity_test_counts}, index=bin_labels)\n",
    "llama2_counts_df.to_csv('analysis_results/binary-top-2k/Token_Count_Analysis/llama2_tokenized_lengths_bins.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_dataset[\"train\"] = multiclass_dataset[\"train\"].remove_columns([\"flanT5_instruction_token_count\", \"llama2_instruction_token_count\"])\n",
    "multiclass_dataset[\"test\"] = multiclass_dataset[\"test\"].remove_columns([\"flanT5_instruction_token_count\", \"llama2_instruction_token_count\"])\n",
    "\n",
    "multiclass_top_2k_dataset[\"train\"] = multiclass_top_2k_dataset[\"train\"].remove_columns([\"flanT5_instruction_token_count\", \"llama2_instruction_token_count\"])\n",
    "multiclass_top_2k_dataset[\"test\"] = multiclass_top_2k_dataset[\"test\"].remove_columns([\"flanT5_instruction_token_count\", \"llama2_instruction_token_count\"])\n",
    "\n",
    "binary_dataset[\"train\"] = binary_dataset[\"train\"].remove_columns([\"flanT5_instruction_token_count\", \"llama2_instruction_token_count\"])\n",
    "binary_dataset[\"test\"] = binary_dataset[\"test\"].remove_columns([\"flanT5_instruction_token_count\", \"llama2_instruction_token_count\"])\n",
    "\n",
    "binary_top_2k_dataset[\"train\"] = binary_top_2k_dataset[\"train\"].remove_columns([\"flanT5_instruction_token_count\", \"llama2_instruction_token_count\"])\n",
    "binary_top_2k_dataset[\"test\"] = binary_top_2k_dataset[\"test\"].remove_columns([\"flanT5_instruction_token_count\", \"llama2_instruction_token_count\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Ambiguity Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ambiguity_scores(dataset, split):\n",
    "\n",
    "    scores = []\n",
    "    for example in dataset[split]:\n",
    "        scores.append(example['ambiguity_score'])\n",
    "    return scores\n",
    "\n",
    "# Calculate train/test ambiguity scores for each dataset\n",
    "multi_ambiguity_scores = {\n",
    "    'train': get_ambiguity_scores(multiclass_dataset, 'train'),\n",
    "    'test': get_ambiguity_scores(multiclass_dataset, 'test')\n",
    "}\n",
    "\n",
    "multi_top_2k_ambiguity_scores = {\n",
    "    'train': get_ambiguity_scores(multiclass_top_2k_dataset, 'train'),\n",
    "    'test': get_ambiguity_scores(multiclass_top_2k_dataset, 'test')\n",
    "}\n",
    "\n",
    "binary_ambiguity_scores = {\n",
    "    'train': get_ambiguity_scores(binary_dataset, 'train'),\n",
    "    'test': get_ambiguity_scores(binary_dataset, 'test')\n",
    "}\n",
    "\n",
    "binary_top_2k_ambiguity_scores = {\n",
    "    'train': get_ambiguity_scores(binary_top_2k_dataset, 'train'),\n",
    "    'test': get_ambiguity_scores(binary_top_2k_dataset, 'test')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([multi_ambiguity_scores['train'], multi_ambiguity_scores['test']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "ambiguity_train_counts, _ = np.histogram(multi_ambiguity_scores['train'], bins=bin_edges)\n",
    "ambiguity_test_counts, _ = np.histogram(multi_ambiguity_scores['test'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(multi_ambiguity_scores['train'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(multi_ambiguity_scores['test'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Multiclass Ambiguity Scores', fontsize=16)\n",
    "plt.xlabel('Score', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/multiclass/Decision_Ambiguity_Analysis/ambiguity_scores.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "llama2_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': ambiguity_train_counts, 'Train': ambiguity_test_counts}, index=bin_labels)\n",
    "llama2_counts_df.to_csv('analysis_results/multiclass/Decision_Ambiguity_Analysis/ambiguity_scores_bins.csv', index=False)\n",
    "\n",
    "\n",
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([multi_top_2k_ambiguity_scores['train'], multi_top_2k_ambiguity_scores['test']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "ambiguity_train_counts, _ = np.histogram(multi_top_2k_ambiguity_scores['train'], bins=bin_edges)\n",
    "ambiguity_test_counts, _ = np.histogram(multi_top_2k_ambiguity_scores['test'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(multi_top_2k_ambiguity_scores['train'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(multi_top_2k_ambiguity_scores['test'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Multiclass-Top-2k Ambiguity Scores', fontsize=16)\n",
    "plt.xlabel('Score', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/multiclass-top-2k/Decision_Ambiguity_Analysis/ambiguity_scores.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "llama2_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': ambiguity_train_counts, 'Train': ambiguity_test_counts}, index=bin_labels)\n",
    "llama2_counts_df.to_csv('analysis_results/multiclass-top-2k/Decision_Ambiguity_Analysis/ambiguity_scores_bins.csv', index=False)\n",
    "\n",
    "\n",
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([binary_ambiguity_scores['train'], binary_ambiguity_scores['test']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "ambiguity_train_counts, _ = np.histogram(binary_ambiguity_scores['train'], bins=bin_edges)\n",
    "ambiguity_test_counts, _ = np.histogram(binary_ambiguity_scores['test'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(binary_ambiguity_scores['train'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(binary_ambiguity_scores['test'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Binary Ambiguity Scores', fontsize=16)\n",
    "plt.xlabel('Score', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/binary/Decision_Ambiguity_Analysis/ambiguity_scores.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "llama2_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': ambiguity_train_counts, 'Train': ambiguity_test_counts}, index=bin_labels)\n",
    "llama2_counts_df.to_csv('analysis_results/binary/Decision_Ambiguity_Analysis/ambiguity_scores_bins.csv', index=False)\n",
    "\n",
    "\n",
    "# Calculate the common bin edges for both train and test submissions\n",
    "combined_data = np.concatenate([binary_top_2k_ambiguity_scores['train'], binary_top_2k_ambiguity_scores['test']])\n",
    "bin_edges = np.histogram_bin_edges(combined_data, bins=20)\n",
    "\n",
    "# Get histogram bin counts for train and test submissions\n",
    "ambiguity_train_counts, _ = np.histogram(binary_top_2k_ambiguity_scores['train'], bins=bin_edges)\n",
    "ambiguity_test_counts, _ = np.histogram(binary_top_2k_ambiguity_scores['test'], bins=bin_edges)\n",
    "\n",
    "# Plot histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(binary_top_2k_ambiguity_scores['train'], bins=bin_edges, kde=True, label='Training Set', color='blue')\n",
    "sns.histplot(binary_top_2k_ambiguity_scores['test'], bins=bin_edges, kde=True, label='Testing Set', color='orange')\n",
    "plt.title('Reddit-AITA-Binary-Top-2k Ambiguity Scores', fontsize=16)\n",
    "plt.xlabel('Score', fontsize=14)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.savefig('analysis_results/binary-top-2k/Decision_Ambiguity_Analysis/ambiguity_scores.png')\n",
    "plt.show()\n",
    "\n",
    "# Log Histogram Bin Counts\n",
    "bin_labels = [f\"{bin_edges[i]:.2f} - {bin_edges[i+1]:.2f}\" for i in range(len(bin_edges)-1)]\n",
    "llama2_counts_df = pd.DataFrame({'Bin Ranges': bin_labels, 'Test': ambiguity_train_counts, 'Train': ambiguity_test_counts}, index=bin_labels)\n",
    "llama2_counts_df.to_csv('analysis_results/binary-top-2k/Decision_Ambiguity_Analysis/ambiguity_scores_bins.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute percentiles for 'train' and test datasets\n",
    "train_percentiles = np.percentile(multi_ambiguity_scores['train'], np.arange(0, 101, 1))\n",
    "test_percentiles = np.percentile(multi_ambiguity_scores['test'], np.arange(0, 101, 1))\n",
    "combined_percentile_data = pd.DataFrame({\n",
    "    \"Percentile\": np.arange(0, 101, 1),\n",
    "    \"Train Ambiguity Score\": train_percentiles,\n",
    "    \"Test Ambiguity Score\": test_percentiles\n",
    "})\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_percentile_data.to_csv(\"analysis_results/multiclass/Decision_Ambiguity_Analysis/ambiguity_score_cumulative_frequency_bins.csv\", index=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "plt.plot(train_percentiles, np.arange(0, 101, 1), label='Train')\n",
    "plt.plot(test_percentiles, np.arange(0, 101, 1), label='Test')\n",
    "plt.xlabel(\"Ambiguity Score\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Reddit-AITA-Multiclass Ambiguity Score Cumulative Frequency\")\n",
    "plt.legend()\n",
    "plt.savefig(\"analysis_results/multiclass/Decision_Ambiguity_Analysis/ambiguity_scores_cumulative_frequency.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compute percentiles for 'train' and test datasets\n",
    "train_percentiles = np.percentile(multi_top_2k_ambiguity_scores['train'], np.arange(0, 101, 1))\n",
    "test_percentiles = np.percentile(multi_top_2k_ambiguity_scores['test'], np.arange(0, 101, 1))\n",
    "combined_percentile_data = pd.DataFrame({\n",
    "    \"Percentile\": np.arange(0, 101, 1),\n",
    "    \"Train Ambiguity Score\": train_percentiles,\n",
    "    \"Test Ambiguity Score\": test_percentiles\n",
    "})\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_percentile_data.to_csv(\"analysis_results/multiclass-top-2k/Decision_Ambiguity_Analysis/ambiguity_score_cumulative_frequency_bins.csv\", index=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "plt.plot(train_percentiles, np.arange(0, 101, 1), label='Train')\n",
    "plt.plot(test_percentiles, np.arange(0, 101, 1), label='Test')\n",
    "plt.xlabel(\"Ambiguity Score\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Reddit-AITA-Multiclass-Top-2k Ambiguity Score Cumulative Frequency\")\n",
    "plt.legend()\n",
    "plt.savefig(\"analysis_results/multiclass-top-2k/Decision_Ambiguity_Analysis/ambiguity_scores_cumulative_frequency.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compute percentiles for 'train' and test datasets\n",
    "train_percentiles = np.percentile(binary_ambiguity_scores['train'], np.arange(0, 101, 1))\n",
    "test_percentiles = np.percentile(binary_ambiguity_scores['test'], np.arange(0, 101, 1))\n",
    "combined_percentile_data = pd.DataFrame({\n",
    "    \"Percentile\": np.arange(0, 101, 1),\n",
    "    \"Train Ambiguity Score\": train_percentiles,\n",
    "    \"Test Ambiguity Score\": test_percentiles\n",
    "})\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_percentile_data.to_csv(\"analysis_results/binary/Decision_Ambiguity_Analysis/ambiguity_score_cumulative_frequency_bins.csv\", index=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "plt.plot(train_percentiles, np.arange(0, 101, 1), label='Train')\n",
    "plt.plot(test_percentiles, np.arange(0, 101, 1), label='Test')\n",
    "plt.xlabel(\"Ambiguity Score\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Reddit-AITA-Binary Ambiguity Score Cumulative Frequency\")\n",
    "plt.legend()\n",
    "plt.savefig(\"analysis_results/binary/Decision_Ambiguity_Analysis/ambiguity_scores_cumulative_frequency.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Compute percentiles for 'train' and test datasets\n",
    "train_percentiles = np.percentile(binary_top_2k_ambiguity_scores['train'], np.arange(0, 101, 1))\n",
    "test_percentiles = np.percentile(binary_top_2k_ambiguity_scores['test'], np.arange(0, 101, 1))\n",
    "combined_percentile_data = pd.DataFrame({\n",
    "    \"Percentile\": np.arange(0, 101, 1),\n",
    "    \"Train Ambiguity Score\": train_percentiles,\n",
    "    \"Test Ambiguity Score\": test_percentiles\n",
    "})\n",
    "\n",
    "# Save the combined DataFrame to a CSV file\n",
    "combined_percentile_data.to_csv(\"analysis_results/binary-top-2k/Decision_Ambiguity_Analysis/ambiguity_score_cumulative_frequency_bins.csv\", index=False)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 6))  # Adjust the figure size as needed\n",
    "plt.plot(train_percentiles, np.arange(0, 101, 1), label='Train')\n",
    "plt.plot(test_percentiles, np.arange(0, 101, 1), label='Test')\n",
    "plt.xlabel(\"Ambiguity Score\")\n",
    "plt.ylabel(\"Percentile\")\n",
    "plt.title(\"Reddit-AITA-Binary-Top-2k Ambiguity Score Cumulative Frequency\")\n",
    "plt.legend()\n",
    "plt.savefig(\"analysis_results/binary-top-2k/Decision_Ambiguity_Analysis/ambiguity_scores_cumulative_frequency.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the datasets to include only samples with an ambiguity score of 0\n",
    "\n",
    "multiclass_zero_ambiguity_dataset = multiclass_dataset.filter(lambda x: x['ambiguity_score'] == 0)\n",
    "multiclass_top_2k_zero_ambiguity_dataset = multiclass_top_2k_dataset.filter(lambda x: x['ambiguity_score'] == 0)\n",
    "binary_zero_ambiguity_dataset = binary_dataset.filter(lambda x: x['ambiguity_score'] == 0)\n",
    "binary_top_2k_zero_ambiguity_dataset = binary_top_2k_dataset.filter(lambda x: x['ambiguity_score'] == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Get counts for each dataset split\n",
    "zero_ambiguity_count = len(multiclass_zero_ambiguity_dataset['train']) + len(multiclass_zero_ambiguity_dataset['test'])\n",
    "\n",
    "# Calculate percentages\n",
    "total_count= len(multiclass_dataset['train']) + len(multiclass_dataset['test'])\n",
    "zero_ambiguity_percentage = round((zero_ambiguity_count / total_count) * 100, 3)\n",
    "\n",
    "# Store results in dataframe and save to output CSV\n",
    "zero_ambiguity_results = {\n",
    "    'Number of Samples with Zero Ambiguity': [zero_ambiguity_count],\n",
    "    'Percentage of Samples with Zero Ambiguity': [zero_ambiguity_percentage]\n",
    "}\n",
    "\n",
    "output_file = \"analysis_results/multiclass/Decision_Ambiguity_Analysis/zero_ambiguity_analysis_results.json\"\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(zero_ambiguity_results, file, indent=4)\n",
    "\n",
    "\n",
    "# Get counts for each dataset split\n",
    "zero_ambiguity_count = len(multiclass_top_2k_zero_ambiguity_dataset['train']) + len(multiclass_top_2k_zero_ambiguity_dataset['test'])\n",
    "\n",
    "# Calculate percentages\n",
    "total_count= len(multiclass_top_2k_dataset['train']) + len(multiclass_top_2k_dataset['test'])\n",
    "zero_ambiguity_percentage = round((zero_ambiguity_count / total_count) * 100, 3)\n",
    "\n",
    "# Store results in dataframe and save to output CSV\n",
    "zero_ambiguity_results = {\n",
    "    'Number of Samples with Zero Ambiguity': [zero_ambiguity_count],\n",
    "    'Percentage of Samples with Zero Ambiguity': [zero_ambiguity_percentage]\n",
    "}\n",
    "\n",
    "output_file = \"analysis_results/multiclass-top-2k/Decision_Ambiguity_Analysis/zero_ambiguity_analysis_results.json\"\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(zero_ambiguity_results, file, indent=4)\n",
    "\n",
    "\n",
    "# Get counts for each dataset split\n",
    "zero_ambiguity_count = len(binary_zero_ambiguity_dataset['train']) + len(binary_zero_ambiguity_dataset['test'])\n",
    "\n",
    "# Calculate percentages\n",
    "total_count= len(binary_dataset['train']) + len(binary_dataset['test'])\n",
    "zero_ambiguity_percentage = round((zero_ambiguity_count / total_count) * 100, 3)\n",
    "\n",
    "# Store results in dataframe and save to output CSV\n",
    "zero_ambiguity_results = {\n",
    "    'Number of Samples with Zero Ambiguity': [zero_ambiguity_count],\n",
    "    'Percentage of Samples with Zero Ambiguity': [zero_ambiguity_percentage]\n",
    "}\n",
    "\n",
    "output_file = \"analysis_results/binary/Decision_Ambiguity_Analysis/zero_ambiguity_analysis_results.json\"\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(zero_ambiguity_results, file, indent=4)\n",
    "\n",
    "\n",
    "# Get counts for each dataset split\n",
    "zero_ambiguity_count = len(binary_top_2k_zero_ambiguity_dataset['train']) + len(binary_top_2k_zero_ambiguity_dataset['test'])\n",
    "\n",
    "# Calculate percentages\n",
    "total_count= len(binary_top_2k_dataset['train']) + len(binary_top_2k_dataset['test'])\n",
    "zero_ambiguity_percentage = round((zero_ambiguity_count / total_count) * 100, 3)\n",
    "\n",
    "# Store results in dataframe and save to output CSV\n",
    "zero_ambiguity_results = {\n",
    "    'Number of Samples with Zero Ambiguity': [zero_ambiguity_count],\n",
    "    'Percentage of Samples with Zero Ambiguity': [zero_ambiguity_percentage]\n",
    "}\n",
    "\n",
    "output_file = \"analysis_results/binary-top-2k/Decision_Ambiguity_Analysis/zero_ambiguity_analysis_results.json\"\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(zero_ambiguity_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment Agreement Analysis\n",
    "- Holistic: Krippendorff's Alpha\n",
    "  - Key aspects\n",
    "    - \"Krippendorff's alpha coefficient,[1] named after academic Klaus Krippendorff, is a statistical measure of the agreement achieved when coding a set of units of analysis. Since the 1970s, alpha has been used in content analysis where textual units are categorized by trained readers, in counseling and survey research where experts code open-ended interview data into analyzable terms, in psychological testing where alternative tests of the same phenomena need to be compared, or in observational studies where unstructured happenings are recorded for subsequent analysis.\"\n",
    "    - \"Krippendorff's alpha generalizes several known statistics, often called measures of inter-coder agreement, inter-rater reliability, reliability of coding given sets of units (as distinct from unitizing) but it also distinguishes itself from statistics that are called reliability coefficients but are unsuitable to the particulars of coding data generated for subsequent analysis.\"\n",
    "    - \"Krippendorff's alpha is applicable to any number of coders, each assigning one value to one unit of analysis, to incomplete (missing) data, to any number of values available for coding a variable, to binary, nominal, ordinal, interval, ratio, polar, and circular metrics (note that this is not a metric in the mathematical sense, but often the square of a mathematical metric, see levels of measurement), and it adjusts itself to small sample sizes of the reliability data. The virtue of a single coefficient with these variations is that computed reliabilities are comparable across any numbers of coders, values, different metrics, and unequal sample sizes.\n",
    "  - [Wiki](https://en.wikipedia.org/wiki/Krippendorff%27s_alpha)\n",
    "  - [Lecture by Krippendorff on Calculation](https://www.asc.upenn.edu/sites/default/files/2021-03/Computing%20Krippendorff%27s%20Alpha-Reliability.pdf)\n",
    "  - [Article Explanation](https://www.surgehq.ai/blog/inter-rater-reliability-metrics-an-introduction-to-krippendorffs-alpha)\n",
    "    - Ranges from -1 to 1, with -1 being complete disagreement, 0 being random choice, and 1 being complete agreement\n",
    "    - 0.8 indicates significant agreement.\n",
    "\n",
    "- Pairwise: Cohen's Kappa\n",
    "  - [Wiki](https://en.wikipedia.org/wiki/Cohen%27s_kappa)\n",
    "  - [Article Explanation](https://towardsdatascience.com/multi-class-metrics-made-simple-the-kappa-score-aka-cohens-kappa-coefficient-bdea137af09c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import krippendorff\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from itertools import combinations\n",
    "\n",
    "def get_encoded_classifications(dataset):\n",
    "    \"\"\"\n",
    "    Encodes AITA classifications into numeric values, retaining None values.\n",
    "\n",
    "    Parameters:\n",
    "    dataset (list of dictionaries): A huggingface dataset\n",
    "\n",
    "    Returns:\n",
    "    list[list]: Lists of numeric classifications, with None where input was None\n",
    "    \"\"\"\n",
    "\n",
    "    # Mapping of AITA classifications to numeric values\n",
    "    classification_values = {'YTA': 1, 'ESH': 2,\n",
    "                             'INFO': 3, 'NAH': 4,\n",
    "                             'NTA': 5}\n",
    "\n",
    "    # Initialize a list of lists, one for each of the top 10 comments\n",
    "    top_comments = [[] for _ in range(10)]\n",
    "\n",
    "    # Iterate over each sample in the dataset\n",
    "    for sample in dataset:\n",
    "        # Iterate over the top 10 comments\n",
    "        for i in range(10):\n",
    "            key = f'top_comment_{i+1}_classification'\n",
    "            # Append the classification to the corresponding list\n",
    "            top_comments[i].append(sample.get(key, None))\n",
    "\n",
    "    # Convert classifications to their numeric representations, keeping None as is\n",
    "    top_comments_encoded = []\n",
    "    for i in range(len(top_comments)):\n",
    "        encoded_comment = [classification_values.get(c, None) for c in top_comments[i]]\n",
    "        top_comments_encoded.append(encoded_comment)\n",
    "    return top_comments_encoded\n",
    "\n",
    "\n",
    "def calculate_krippendorffs_alpha(dataset):\n",
    "  \"\"\"\n",
    "  Calculates Krippendorff's alpha for a given dataset.\n",
    "\n",
    "  Parameters:\n",
    "  dataset (list of dictionaries): A huggingface dataset.\n",
    "\n",
    "  Returns:\n",
    "  float: Krippendorff's alpha score.\n",
    "  \"\"\"\n",
    "\n",
    "  # Encode top comment classifications\n",
    "  top_comments_encoded = get_encoded_classifications(dataset)\n",
    "\n",
    "  # Calculate and return krippendorff's alpha\n",
    "  data = np.array([[np.nan if x is None else x for x in sublist] for sublist in top_comments_encoded], dtype=float)\n",
    "  return krippendorff.alpha(data)\n",
    "\n",
    "\n",
    "def calculate_cohen_kappa(dataset):\n",
    "  \"\"\"\n",
    "  Calculates Cohen's Kappa score for a given dataset.\n",
    "\n",
    "  Parameters:\n",
    "  dataset (list of dictionaries): A huggingface dataset.\n",
    "\n",
    "  Returns:\n",
    "  dict: A dictionary of Cohen's Kappa scores for each pair of top comments.\n",
    "  \"\"\"\n",
    "\n",
    "  # encode top comment classifications\n",
    "  top_comments_encoded = get_encoded_classifications(dataset)\n",
    "\n",
    "  scores = {}\n",
    "  for list1, list2 in combinations(top_comments_encoded, 2):\n",
    "      filtered_list1 = []\n",
    "      filtered_list2 = []\n",
    "      for true, pred in zip(list1, list2):\n",
    "          if true is not None and pred is not None:\n",
    "              filtered_list1.append(true)\n",
    "              filtered_list2.append(pred)\n",
    "      score = cohen_kappa_score(filtered_list1, filtered_list2)\n",
    "      index1 = top_comments_encoded.index(list1)\n",
    "      index2 = top_comments_encoded.index(list2)\n",
    "      key = (f\"top_comment_{index1 + 1}\", f\"top_comment_{index2 + 1}\")\n",
    "      scores[key] = score\n",
    "  return scores\n",
    "\n",
    "\n",
    "def save_cohen_kappa_scores(cohen_kappa_scores, output_file):\n",
    "  \"\"\"\n",
    "  Saves Cohen's Kappa scores to a CSV file.\n",
    "\n",
    "  Parameters:\n",
    "  cohen_kappa_scores (dict): A dictionary of Cohen's Kappa scores.\n",
    "  output_file (str): The path to the output CSV file.\n",
    "  \"\"\"\n",
    "\n",
    "  # create a list of column and row names\n",
    "  comments = [f\"top_comment_{i}\" for i in range(1, 11)]\n",
    "\n",
    "  # create an empty dataframe and fill with scores\n",
    "  df = pd.DataFrame(index=comments, columns=comments)\n",
    "  for (comment1, comment2), score in cohen_kappa_scores.items():\n",
    "      df.at[comment1, comment2] = round(score, 3)\n",
    "\n",
    "  # set the lower triangle to NaN, including the diagonal\n",
    "  for i in range(len(df)):\n",
    "      for j in range(i + 1):\n",
    "          df.iat[i, j] = np.nan\n",
    "\n",
    "  # save the dataframe\n",
    "  df.to_csv(output_file, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Calculate and save Krippendorff's alphas for both train and test datasets\n",
    "krippendorffs_alpha = {\n",
    "    'train': calculate_krippendorffs_alpha(multiclass_dataset['train']),\n",
    "    'test': calculate_krippendorffs_alpha(multiclass_dataset['test'])\n",
    "}\n",
    "output_file_alpha = \"analysis_results/multiclass/Comment_Agreement_Analysis/krippendorffs_alpha.json\"\n",
    "with open(output_file_alpha, \"w\") as f:\n",
    "    json.dump(krippendorffs_alpha, f)\n",
    "\n",
    "# Calculate and save Cohen's Kappa scores for both train and test datasets\n",
    "cohen_kappa_scores = {\n",
    "    'train': {str(key): value for key, value in calculate_cohen_kappa(multiclass_dataset['train']).items()},\n",
    "    'test': {str(key): value for key, value in calculate_cohen_kappa(multiclass_dataset['test']).items()}\n",
    "}\n",
    "output_file_kappa = \"analysis_results/multiclass/Comment_Agreement_Analysis/cohen_kappa_scores.json\"\n",
    "with open(output_file_kappa, \"w\") as f:\n",
    "    json.dump(cohen_kappa_scores, f)\n",
    "\n",
    "\n",
    "# Calculate and save Krippendorff's alphas for both train and test datasets\n",
    "krippendorffs_alpha = {\n",
    "    'train': calculate_krippendorffs_alpha(multiclass_top_2k_dataset['train']),\n",
    "    'test': calculate_krippendorffs_alpha(multiclass_top_2k_dataset['test'])\n",
    "}\n",
    "output_file_alpha = \"analysis_results/multiclass-top-2k/Comment_Agreement_Analysis/krippendorffs_alpha.json\"\n",
    "with open(output_file_alpha, \"w\") as f:\n",
    "    json.dump(krippendorffs_alpha, f)\n",
    "\n",
    "# Calculate and save Cohen's Kappa scores for both train and test datasets\n",
    "cohen_kappa_scores = {\n",
    "    'train': {str(key): value for key, value in calculate_cohen_kappa(multiclass_top_2k_dataset['train']).items()},\n",
    "    'test': {str(key): value for key, value in calculate_cohen_kappa(multiclass_top_2k_dataset['test']).items()}\n",
    "}\n",
    "output_file_kappa = \"analysis_results/multiclass-top-2k/Comment_Agreement_Analysis/cohen_kappa_scores.json\"\n",
    "with open(output_file_kappa, \"w\") as f:\n",
    "    json.dump(cohen_kappa_scores, f)\n",
    "\n",
    "\n",
    "# Calculate and save Krippendorff's alphas for both train and test datasets\n",
    "krippendorffs_alpha = {\n",
    "    'train': calculate_krippendorffs_alpha(binary_dataset['train']),\n",
    "    'test': calculate_krippendorffs_alpha(binary_dataset['test'])\n",
    "}\n",
    "output_file_alpha = \"analysis_results/binary/Comment_Agreement_Analysis/krippendorffs_alpha.json\"\n",
    "with open(output_file_alpha, \"w\") as f:\n",
    "    json.dump(krippendorffs_alpha, f)\n",
    "\n",
    "# Calculate and save Cohen's Kappa scores for both train and test datasets\n",
    "cohen_kappa_scores = {\n",
    "    'train': {str(key): value for key, value in calculate_cohen_kappa(binary_dataset['train']).items()},\n",
    "    'test': {str(key): value for key, value in calculate_cohen_kappa(binary_dataset['test']).items()}\n",
    "}\n",
    "output_file_kappa = \"analysis_results/binary/Comment_Agreement_Analysis/cohen_kappa_scores.json\"\n",
    "with open(output_file_kappa, \"w\") as f:\n",
    "    json.dump(cohen_kappa_scores, f)\n",
    "\n",
    "\n",
    "# Calculate and save Krippendorff's alphas for both train and test datasets\n",
    "krippendorffs_alpha = {\n",
    "    'train': calculate_krippendorffs_alpha(binary_top_2k_dataset['train']),\n",
    "    'test': calculate_krippendorffs_alpha(binary_top_2k_dataset['test'])\n",
    "}\n",
    "output_file_alpha = \"analysis_results/binary-top-2k/Comment_Agreement_Analysis/krippendorffs_alpha.json\"\n",
    "with open(output_file_alpha, \"w\") as f:\n",
    "    json.dump(krippendorffs_alpha, f)\n",
    "\n",
    "# Calculate and save Cohen's Kappa scores for both train and test datasets\n",
    "cohen_kappa_scores = {\n",
    "    'train': {str(key): value for key, value in calculate_cohen_kappa(binary_top_2k_dataset['train']).items()},\n",
    "    'test': {str(key): value for key, value in calculate_cohen_kappa(binary_top_2k_dataset['test']).items()}\n",
    "}\n",
    "output_file_kappa = \"analysis_results/binary-top-2k/Comment_Agreement_Analysis/cohen_kappa_scores.json\"\n",
    "with open(output_file_kappa, \"w\") as f:\n",
    "    json.dump(cohen_kappa_scores, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
