{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AITA Dataset Processing\n",
    "Creates and saves four datasets to HuggingFace Hub\n",
    "- Multi-class top 50k\n",
    "- Multi-class top 2k\n",
    "- Binary (samples with YTA or NTA classification in multi-class top 50k)\n",
    "- Binary top 2k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install datasets transformers pandas numpy krippendorff huggingface_hub ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to the path of the dataset_creation folder on your machine\n",
    "%cd C:\\Users\\mattb\\Documents\\Github\\Reddit_AITA_Finetuning\\dataset_creation\n",
    "!mkdir cleaning_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading of Initial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "initial_datafile = '2019_to_2022_submissions_at_least_50_score_top_10_comments.csv'  # change to data file path\n",
    "dataset = Dataset.from_pandas(pd.read_csv(initial_datafile))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Samples where Top Comment Doesn't Begin with an AITA Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# regex pattern that matches rows where 'top_comment_1' starts with 'nta', 'yta', 'esh', 'info', or 'nah'\n",
    "regex = re.compile(r'^(nta|yta|esh|info|nah)', re.IGNORECASE)\n",
    "\n",
    "# function to apply the regex filter\n",
    "def filter_rows(example):\n",
    "    return bool(regex.match(example['top_comment_1']))\n",
    "\n",
    "# filter the dataset using the regex pattern\n",
    "filtered_dataset = dataset.filter(filter_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Save AITA classification prefix filtering results\n",
    "\n",
    "rows_removed = dataset.num_rows - filtered_dataset.num_rows\n",
    "percent_change = (filtered_dataset.num_rows - dataset.num_rows) / dataset.num_rows * 100\n",
    "\n",
    "AITA_class_prefix_filtering_results = {\n",
    "    \"number of samples before filtering\": dataset.num_rows,\n",
    "    \"number of samples after filtering\": filtered_dataset.num_rows,\n",
    "    \"number of samples removed\": rows_removed,\n",
    "    \"percent change in number of samples\": percent_change,\n",
    "}\n",
    "\n",
    "output_file = \"processing_results/AITA_prefix_filtering_results.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(AITA_class_prefix_filtering_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = filtered_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Edits in both Submission Texts and Top Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_edits(text):\n",
    "  \"\"\"\n",
    "  Removes the edits portion of a text\n",
    "\n",
    "  Parameters:\n",
    "    text: A string containing the text.\n",
    "\n",
    "  Returns:\n",
    "    A string with the edits removed, if present.\n",
    "  \"\"\"\n",
    "\n",
    "  global edits_removed_counter\n",
    "\n",
    "  if text == None:\n",
    "    return text\n",
    "\n",
    "  text = text.lower()\n",
    "\n",
    "  pattern = r\"(edit:|edit -|edit-|eta:|eta -|eta-|edited:|edited -|edited-|edit after:|edit after- |edit after -|edit afterwards:|edit afterwards -|edit afterwards-|edited to add:|edited to add -|edited to add-|update:|update-|update -|updated:|updated-|updated -)\"\n",
    "  match = re.search(pattern, text, flags=re.IGNORECASE)\n",
    "  if match:\n",
    "      edits_removed_counter += 1 # increment the edits_removed_counter\n",
    "      return text[:match.start()].strip() # return the text up to the start of the match\n",
    "\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avg_length(strings):\n",
    "  \"\"\"\n",
    "  Calculates the average length of a list of strings.\n",
    "\n",
    "  Args:\n",
    "    strings (list): A list of strings.\n",
    "\n",
    "  Returns:\n",
    "    float: The average length of the strings.\n",
    "  \"\"\"\n",
    "\n",
    "  filtered_strings = [s for s in strings if s is not None] # filter out None values\n",
    "  total_length = sum(len(s) for s in filtered_strings)\n",
    "  average_length = total_length / len(filtered_strings) if filtered_strings else 0\n",
    "  return average_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "# create the results dictionary\n",
    "\n",
    "edits_removal_results = {\n",
    "    'submission_texts': defaultdict(list),\n",
    "    'top_comment_1': defaultdict(list),\n",
    "    'top_comment_2': defaultdict(list),\n",
    "    'top_comment_3': defaultdict(list),\n",
    "    'top_comment_4': defaultdict(list),\n",
    "    'top_comment_5': defaultdict(list),\n",
    "    'top_comment_6': defaultdict(list),\n",
    "    'top_comment_7': defaultdict(list),\n",
    "    'top_comment_8': defaultdict(list),\n",
    "    'top_comment_9': defaultdict(list),\n",
    "    'top_comment_10': defaultdict(list),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add average lengths to result dictionary before removing edits\n",
    "\n",
    "texts_with_potential_edits = {\n",
    "    'submission_texts': dataset[\"submission_text\"],\n",
    "    'top_comment_1': dataset[\"top_comment_1\"],\n",
    "    'top_comment_2': dataset[\"top_comment_2\"],\n",
    "    'top_comment_3': dataset[\"top_comment_3\"],\n",
    "    'top_comment_4': dataset[\"top_comment_4\"],\n",
    "    'top_comment_5': dataset[\"top_comment_5\"],\n",
    "    'top_comment_6': dataset[\"top_comment_6\"],\n",
    "    'top_comment_7': dataset[\"top_comment_7\"],\n",
    "    'top_comment_8': dataset[\"top_comment_8\"],\n",
    "    'top_comment_9': dataset[\"top_comment_9\"],\n",
    "    'top_comment_10': dataset[\"top_comment_10\"],\n",
    "}\n",
    "\n",
    "for key, texts in texts_with_potential_edits.items():\n",
    "    edits_removal_results[key]['avg_length_before_removing_edits'] = get_avg_length(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove edits for submissions and comments\n",
    "\n",
    "edits_removed_counter = 0\n",
    "\n",
    "# submission texts\n",
    "dataset = dataset.map(lambda x: {\"submission_text\": remove_edits(x[\"submission_text\"])})\n",
    "edits_removal_results['submission_texts']['edits_removed'] = edits_removed_counter\n",
    "\n",
    "# comments\n",
    "for i in range(1, 11):\n",
    "    edits_removed_counter = 0\n",
    "    dataset = dataset.map(lambda x: {f\"top_comment_{i}\": remove_edits(x[f\"top_comment_{i}\"])})\n",
    "    edits_removal_results[f\"top_comment_{i}\"]['edits_removed'] = edits_removed_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add string lengths to result dictionary after removing edits\n",
    "\n",
    "texts_with_potential_edits = {\n",
    "    'submission_texts': dataset[\"submission_text\"],\n",
    "    'top_comment_1': dataset[\"top_comment_1\"],\n",
    "    'top_comment_2': dataset[\"top_comment_2\"],\n",
    "    'top_comment_3': dataset[\"top_comment_3\"],\n",
    "    'top_comment_4': dataset[\"top_comment_4\"],\n",
    "    'top_comment_5': dataset[\"top_comment_5\"],\n",
    "    'top_comment_6': dataset[\"top_comment_6\"],\n",
    "    'top_comment_7': dataset[\"top_comment_7\"],\n",
    "    'top_comment_8': dataset[\"top_comment_8\"],\n",
    "    'top_comment_9': dataset[\"top_comment_9\"],\n",
    "    'top_comment_10': dataset[\"top_comment_10\"],\n",
    "}\n",
    "\n",
    "for key, texts in texts_with_potential_edits.items():\n",
    "    edits_removal_results[key]['avg_length_after_removing_edits'] = get_avg_length(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add percent changes in average lengths from removing edits\n",
    "\n",
    "def calculate_percent_change(before, after):\n",
    "    if before == 0:\n",
    "        return 0\n",
    "    return ((after - before) / before) * 100\n",
    "\n",
    "for key in edits_removal_results.keys():\n",
    "    before = edits_removal_results[key]['avg_length_before_removing_edits']\n",
    "    after = edits_removal_results[key]['avg_length_after_removing_edits']\n",
    "    percent_change = calculate_percent_change(before, after)\n",
    "    edits_removal_results[key]['avg_length_percent_change'] = percent_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save edits removal results\n",
    "\n",
    "output_file = \"processing_results/edits_removal_results.json\"\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(edits_removal_results, file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removal of Upper Extreme Outliers\n",
    "Two-step filtering process:\n",
    "1. Removal of Samples with submissions that are top 5% of flanT5 token count\n",
    "2. Removal of Samples with #1 comments that are top 5% in flanT5 token count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizer, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "def add_token_counts_to_dataset(dataset: Dataset, column: str, tokenizer: PreTrainedTokenizer, new_column_name: str) -> Dataset:\n",
    "    \"\"\"\n",
    "    Adds a new column to a specified partition of a dataset with the number of tokens in each row of a specified column.\n",
    "\n",
    "    Parameters:\n",
    "      dataset (Dataset): A Hugging Face dataset object.\n",
    "      column (str): The name of the column in the dataset partition to process.\n",
    "      tokenizer: A Hugging Face transformers pretrained tokenizer\n",
    "      new_column_name (str): The name of the new column to be added to the dataset.\n",
    "\n",
    "    Returns:\n",
    "      Dataset: The modified dataset with an additional column for token counts.\n",
    "    \"\"\"\n",
    "\n",
    "    def count_tokens(row):\n",
    "        row_tokens = tokenizer(row[column], padding=False, truncation=False, return_tensors=\"pt\")\n",
    "        tokens_count = len([tensor.item() for tensor in row_tokens['input_ids'][0]])\n",
    "        return {new_column_name: tokens_count}\n",
    "    \n",
    "    return dataset.map(count_tokens)\n",
    "\n",
    "flanT5_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xl\", trust_remote_code=True)\n",
    "\n",
    "dataset = add_token_counts_to_dataset(dataset, 'submission_text', flanT5_tokenizer, 'submission_text_token_count')\n",
    "dataset = add_token_counts_to_dataset(dataset, 'top_comment_1', flanT5_tokenizer, 'top_comment_1_token_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "OUTLIER_PERCENTILE_THRESHOLD = 95\n",
    "\n",
    "def filter_submission_upper_outliers(example):\n",
    "    return example['submission_text_token_count'] <= submission_text_length_threshold\n",
    "\n",
    "def filter_comment_upper_outliers(example):\n",
    "    return example['top_comment_1_token_count'] <= top_comment_length_threshold\n",
    "\n",
    "# Extract lengths into lists\n",
    "submission_text_token_counts = dataset['submission_text_token_count']  \n",
    "top_comment_token_counts = dataset['top_comment_1_token_count']     \n",
    "\n",
    "# Calculate the outlier percentile thresholds\n",
    "submission_text_length_threshold = np.percentile(submission_text_token_counts, OUTLIER_PERCENTILE_THRESHOLD)\n",
    "top_comment_length_threshold = np.percentile(top_comment_token_counts, OUTLIER_PERCENTILE_THRESHOLD)\n",
    "\n",
    "# Filter the dataset\n",
    "rows_before = dataset.num_rows\n",
    "dataset = dataset.filter(filter_submission_upper_outliers)\n",
    "dataset = dataset.filter(filter_comment_upper_outliers)\n",
    "rows_after = dataset.num_rows\n",
    "\n",
    "percent_change = (rows_after - rows_before) / rows_before * 100\n",
    "\n",
    "# Save outlier filtering results\n",
    "outlier_filtering_results ={\n",
    "    \"number of samples before filtering\": rows_before,\n",
    "    \"number of samples after filtering\": rows_after,\n",
    "    \"number of samples removed\": rows_before - rows_after,\n",
    "    \"percent change in number of samples\": percent_change,\n",
    "}\n",
    "\n",
    "output_file = \"processing_results/outlier_filtering_results.json\"\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(outlier_filtering_results, f)\n",
    "\n",
    "# Remove the length columns\n",
    "dataset = dataset.remove_columns(['submission_text_token_count', 'top_comment_1_token_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding of Top Comment AITA Classifications and Ambiguity Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def find_earliest_classification(text):\n",
    "    '''\n",
    "    Find the earliest AITA classification in a text.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to search for AITA classifications in.\n",
    "\n",
    "    Returns:\n",
    "        str: The earliest classification found in the text.\n",
    "    '''\n",
    "\n",
    "    # classifications mapped to their keywords\n",
    "    classes_dictionary = {\n",
    "      'NTA': ['not the asshole', 'not the a\\*\\*hole', 'nta', 'you would not be the asshole', 'you would not be the a**hole', 'ywnbta', 'n t a', 'y w b t a'],\n",
    "      'NAH': ['no assholes here', 'no a\\*\\*holes here', 'nah', 'n a h'],\n",
    "      'ESH': ['everyone sucks here', 'esh', 'e s h'],\n",
    "      'INFO': ['more information needed', 'more info needed', 'more information required', 'more info required', 'info'],\n",
    "      'YTA': ['you\\'re the asshole', 'you\\'re the a\\*\\*hole', 'youre the asshole', 'youre the a\\*\\*hole', 'yta', 'you would be the asshole', 'you would be the a\\*\\*hole', 'ywbta', 'y t a', 'y w b t a']\n",
    "    }\n",
    "\n",
    "    # track earliest match\n",
    "    earliest_match = None\n",
    "    earliest_match_pos = float('inf')  # Initially set to infinity\n",
    "\n",
    "    # convert input text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # go through all classifications and their keywords\n",
    "    for key, phrases in classes_dictionary.items():\n",
    "        # Create a regex pattern that includes the classification keywords\n",
    "        pattern = r'\\b(' + '|'.join(map(re.escape, phrases)) + r')\\b'\n",
    "\n",
    "        # Search for any keywords in the input text\n",
    "        for match in re.finditer(pattern, text, re.IGNORECASE):\n",
    "            if match.start() < earliest_match_pos:\n",
    "                # Update the earliest match if this match is earlier\n",
    "                earliest_match = key\n",
    "                earliest_match_pos = match.start()\n",
    "\n",
    "    # return the class that had the earliest match\n",
    "    return earliest_match\n",
    "\n",
    "def add_classification(row):\n",
    "    '''\n",
    "    Add comment AITA classifications to a row in the datset.\n",
    "\n",
    "    Args:\n",
    "        row (dict): A row from the dataset.\n",
    "\n",
    "    Returns:\n",
    "        dict: The row with comment AITA classifications added.\n",
    "    '''\n",
    "    # Iterate over top 10 comment keys\n",
    "    for i in range(1, 11):\n",
    "        key = f'top_comment_{i}'\n",
    "        if key in row and isinstance(row[key], str):\n",
    "            # if this row has a top_comment_N key, get the classification and add it to the row\n",
    "            classification = find_earliest_classification(row[key])\n",
    "            row[key + '_classification'] = classification\n",
    "        else:\n",
    "            # If the top_comment_N key doesn't exist, skip setting this key\n",
    "            row[key + '_classification'] = None\n",
    "\n",
    "    # return the row with the classification added\n",
    "    return row\n",
    "\n",
    "def calculate_ambiguity(classifications):\n",
    "    '''\n",
    "    Calculate the ambiguity score for a list of classifications.\n",
    "\n",
    "    Args:\n",
    "        classifications (list): A list of classifications.\n",
    "\n",
    "    Returns:\n",
    "        float: The ambiguity score.\n",
    "    '''\n",
    "    classification_values = {'YTA': 1, 'ESH': 2,\n",
    "                             'INFO': 3, 'NAH': 4,\n",
    "                             'NTA': 5}\n",
    "\n",
    "    # convert classifications to their numeric representations\n",
    "    numeric_values = [classification_values[c] for c in classifications if c is not None]\n",
    "\n",
    "    # calculate ambiguity score as a function of mean and std dev\n",
    "    mean = np.mean(numeric_values)\n",
    "    std_dev = np.std(numeric_values)\n",
    "    def f(mean):\n",
    "        return (2 - abs(3 - mean)) ** 2 # parabolic that is lowest when mean is 1 or 5 and highest at 3 to emphasize ambiguity for YTA & NTA classes\n",
    "    ambiguity_score = std_dev * f(mean)\n",
    "\n",
    "    # normalize the ambiguity score on a 0-1 scale\n",
    "    min_score = 0  # Minimum possible score (when std dev equals 0)\n",
    "    max_score = 8.0  # Maximum possible score (when classes are equally YTA and NTA which results in max std dev and a central mean)\n",
    "    normalized_score = (ambiguity_score - min_score) / (max_score - min_score)\n",
    "\n",
    "    # return normalized ambiguity score\n",
    "    return normalized_score\n",
    "\n",
    "def add_ambiguity_score(row):\n",
    "    # extract classifications from top comments\n",
    "    classifications = []\n",
    "    for i in range(1, 11):  # Adjust the range based on the number of top comments\n",
    "        classification_key = f'top_comment_{i}_classification'\n",
    "        if classification_key in row and row[classification_key]:\n",
    "            classifications.append(row[classification_key])\n",
    "\n",
    "    # calculate the ambiguity score if there are classifications\n",
    "    if classifications:\n",
    "        row['ambiguity_score'] = calculate_ambiguity(classifications)\n",
    "    else:\n",
    "        row['ambiguity_score'] = None\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add top comment classifications to dataset\n",
    "dataset = dataset.map(add_classification)\n",
    "\n",
    "# convert dataset to dataframe for null filtering\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# remove the rows where the top 1 comment classification is None\n",
    "rows_before = df.shape[0]\n",
    "df_filtered = df[df['top_comment_1_classification'].notnull()]\n",
    "rows_after = df_filtered.shape[0]\n",
    "rows_removed = rows_before - rows_after\n",
    "percent_change = (rows_removed / rows_before) * 100\n",
    "\n",
    "# save results of filtering out rows with null top comment classifications\n",
    "top_comment_classification_null_filtering_results = {\n",
    "    \"number of samples before filtering\": rows_before,\n",
    "    \"number of samples after filtering\": rows_after,\n",
    "    \"number of samples removed\": rows_removed,\n",
    "    \"percent change in number of samples\": percent_change,\n",
    "}\n",
    "\n",
    "output_file = \"processing_results/top_comment_classification_null_filtering_results.json\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(top_comment_classification_null_filtering_results, f)\n",
    "\n",
    "# convert dataframe back to a dataset\n",
    "dataset = Dataset.from_pandas(df_filtered)\n",
    "\n",
    "# add ambiguity scores to dataset\n",
    "dataset = dataset.map(add_ambiguity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda example: {'AITA_decision': example['decision']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import ClassLabel\n",
    "\n",
    "# Create a mapping for the unique decision strings to integer labels\n",
    "unique_labels = sorted(set(dataset['decision']))\n",
    "label_to_id = {label: id for id, label in enumerate(unique_labels)}\n",
    "\n",
    "# Define a function to map each decision to its integer label\n",
    "def add_decision_class_label(example):\n",
    "    example['decision_class_label'] = label_to_id[example['decision']]\n",
    "    return example\n",
    "\n",
    "# Add the 'decision_class_label' column to the dataset\n",
    "dataset = dataset.map(add_decision_class_label)\n",
    "\n",
    "# Update the features of the dataset to include 'decision_class_label'\n",
    "new_features = dataset.features.copy()\n",
    "new_features['decision_class_label'] = ClassLabel(names=unique_labels)\n",
    "dataset = dataset.cast(new_features)\n",
    "\n",
    "# Remove the original 'decision' column\n",
    "dataset = dataset.remove_columns('decision')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_aita_decision(sample):\n",
    "    # Check if 'AITA_decision' is 'Asshole' and update it\n",
    "    if sample['AITA_decision'] == 'Asshole':\n",
    "        sample['AITA_decision'] = 'A-hole'\n",
    "    return sample\n",
    "\n",
    "dataset = dataset.map(update_aita_decision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(\n",
    "    test_size = 0.2,\n",
    "    stratify_by_column='decision_class_label',\n",
    "    seed=42 \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding of Flan-T5 and Llama-2 Instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from AITA_instruction import AITA_Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding of Flan-T5 multiclass instructions\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    lambda sample: AITA_Instruction.get_flanT5_instruction(sample, instruction_type=\"multiclass\"), \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "dataset[\"test\"] = dataset[\"test\"].map(\n",
    "    lambda sample: AITA_Instruction.get_flanT5_instruction(sample, instruction_type=\"multiclass\"), \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "# adding of Llama 2 multiclass instructions\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    lambda sample: AITA_Instruction.get_llama2_training_instruction(sample, instruction_type=\"multiclass\", partition=\"training\"), \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "dataset[\"test\"] = dataset[\"test\"].map(\n",
    "    lambda sample: AITA_Instruction.get_llama2_training_instruction(sample, instruction_type=\"multiclass\", partition=\"testing\"), \n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[\"train\"] = dataset[\"train\"].remove_columns([\"__index_level_0__\", \"decision_class_label\"])\n",
    "dataset[\"test\"] = dataset[\"test\"].remove_columns([\"__index_level_0__\", \"decision_class_label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving of Multi-class Datasets to HuggingFace Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main dataset (Top 50k)\n",
    "- top 50k by submission score\n",
    "- 80/20 train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Sort the 'train' subset by 'submission_score' and select the top 40000 rows\n",
    "sorted_train = dataset['train'].sort('submission_score', reverse=True)\n",
    "top_train = sorted_train.select(range(40000))\n",
    "\n",
    "# Sort the 'test' subset by 'submission_score' and select the top 10000 rows\n",
    "sorted_test = dataset['test'].sort('submission_score', reverse=True)\n",
    "top_test = sorted_test.select(range(10000))\n",
    "\n",
    "shuffled_train = top_train.shuffle(seed=42)\n",
    "shuffled_test = top_test.shuffle(seed=42)\n",
    "\n",
    "# Create a new DatasetDict with the filtered data\n",
    "dataset = DatasetDict({\"train\": top_train, \"test\": top_test})\n",
    "dataset =  dataset.remove_columns([\"AITA_decision\"])\n",
    "\n",
    "dataset.push_to_hub(f'MattBoraske/reddit-AITA-submissions-and-comments-multiclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 2k Dataset\n",
    "- 80/20 train/test split\n",
    "- Equal representation for each of the five AITA classes\n",
    "    - Top 320/80 training/testing for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def filter_top_samples_df(dataset, top_n):\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Group by 'decision_class_label', sort within groups by 'submission_score', and take top N\n",
    "    grouped = df.groupby('top_comment_1_classification', group_keys=False).apply(lambda x: x.nlargest(top_n, 'submission_score'))\n",
    "    return Dataset.from_pandas(grouped)\n",
    "\n",
    "# Filter the datasets and convert to DataFrames\n",
    "filtered_train_df = filter_top_samples_df(dataset['train'], 320)\n",
    "filtered_test_df = filter_top_samples_df(dataset['test'], 80)\n",
    "\n",
    "shuffled_train = filtered_train_df.shuffle(seed=42)\n",
    "shuffled_test = filtered_test_df.shuffle(seed=42)\n",
    "\n",
    "# Create a new DatasetDict\n",
    "samples_2000_dataset = DatasetDict({\n",
    "    'train': shuffled_train,\n",
    "    'test': shuffled_test\n",
    "})\n",
    "\n",
    "samples_2000_dataset[\"train\"] = samples_2000_dataset[\"train\"].remove_columns([\"__index_level_0__\"])\n",
    "samples_2000_dataset[\"test\"] = samples_2000_dataset[\"test\"].remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "samples_2000_dataset.push_to_hub('MattBoraske/reddit-AITA-submissions-and-comments-multiclass-top-2k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving of Binary Classification Datasets (NTA/YTA only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Dataset (NTA/YTA in Multi-class 50k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('MattBoraske/reddit-AITA-submissions-and-comments')\n",
    "dataset = dataset.remove_columns([\"flanT5_instruction\", \"llama2_instruction\"]) # removing of Flan-T5 and Llama 2 multiclass instructions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get AITA_Instruction custom class to generate binary classification flan-t5/llama-2 instructions\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from AITA_instruction import AITA_Instruction \n",
    "\n",
    "# adding of Flan-T5 binary classification instructions\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    lambda sample: AITA_Instruction.get_flanT5_instruction(sample, instruction_type=\"binary\"), \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "dataset[\"test\"] = dataset[\"test\"].map(\n",
    "    lambda sample: AITA_Instruction.get_flanT5_instruction(sample, instruction_type=\"binary\"), \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "# adding of Llama 2 binary classification instructions\n",
    "\n",
    "dataset[\"train\"] = dataset[\"train\"].map(\n",
    "    lambda sample: AITA_Instruction.get_llama2_training_instruction(sample, instruction_type=\"binary\", partition=\"training\"), \n",
    "    batched=False\n",
    ")\n",
    "\n",
    "dataset[\"test\"] = dataset[\"test\"].map(\n",
    "    lambda sample: AITA_Instruction.get_llama2_training_instruction(sample, instruction_type=\"binary\", partition=\"testing\"), \n",
    "    batched=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter for rows that either have a top comment classification of 'YTA' or 'NTA' and save dataset to HF hub\n",
    "\n",
    "def filter_rows(example):\n",
    "    return example['top_comment_1_classification'] in ['YTA', 'NTA']\n",
    "\n",
    "dataset = {split: ds.filter(filter_rows) for split, ds in dataset.items()}\n",
    "dataset = DatasetDict(dataset)\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "dataset.push_to_hub('MattBoraske/reddit-AITA-submissions-and-comments-binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top 2k dataset\n",
    "- 80/20 train/test split\n",
    "- Equal representation of NTA and YTA classes\n",
    "    - Top 800/200 training/testing for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_top_samples_df(dataset, top_n):\n",
    "    # Convert to Pandas DataFrame\n",
    "    df = dataset.to_pandas()\n",
    "\n",
    "    # Group by 'decision_class_label', sort within groups by 'submission_score', and take top N\n",
    "    grouped = df.groupby('top_comment_1_classification', group_keys=False).apply(lambda x: x.nlargest(top_n, 'submission_score'))\n",
    "    return Dataset.from_pandas(grouped)\n",
    "\n",
    "# Filter the datasets and convert to DataFrames\n",
    "filtered_train_df = filter_top_samples_df(dataset['train'], 800)\n",
    "filtered_test_df = filter_top_samples_df(dataset['test'], 200)\n",
    "\n",
    "shuffled_train = filtered_train_df.shuffle(seed=42)\n",
    "shuffled_test = filtered_test_df.shuffle(seed=42)\n",
    "\n",
    "# Create a new DatasetDict\n",
    "samples_2000_dataset = DatasetDict({\n",
    "    'train': shuffled_train,\n",
    "    'test': shuffled_test\n",
    "})\n",
    "\n",
    "samples_2000_dataset[\"train\"] = samples_2000_dataset[\"train\"].remove_columns([\"__index_level_0__\"])\n",
    "samples_2000_dataset[\"test\"] = samples_2000_dataset[\"test\"].remove_columns([\"__index_level_0__\"])\n",
    "\n",
    "samples_2000_dataset.push_to_hub('MattBoraske/reddit-AITA-submissions-and-comments-binary-top-2k')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
